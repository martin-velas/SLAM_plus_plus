/*
								+---------------------------------+
								|                                 |
								|  ***   Über Block Matrix   ***  |
								|                                 |
								| Copyright  (c) -tHE SWINe- 2012 |
								|                                 |
								|          BlockMatrix.h          |
								|                                 |
								+---------------------------------+
*/

#pragma once
#ifndef __UBER_BLOCK_MATRIX_INCLUDED
#define __UBER_BLOCK_MATRIX_INCLUDED

/**
 *	@file include/slam/BlockMatrix.h
 *	@date 2012
 *	@author -tHE SWINe-
 *	@brief the überblockmatrix class interface
 *
 *	The überblockmatrix uses a special referencing allocator,
 *	which, in fact, does not allocate anything itself. It only
 *	holds a reference to data allocated elsewhere and remembers
 *	matrix dimensions. While this is very elegant solution to
 *	the underlying problem, Eigen error checking does not allow
 *	that. Either \#define EIGEN_NO_DEBUG and EIGEN_NO_STATIC_ASSERT
 *	to disable compile-time checks in Eigen (but you lose some
 *	nice debugging functionality), or add Use_ReferencingAllocator
 *	next to Eigen::ColMajor and Eigen::AutoAlign at the end in
 *	Eigen::PlainObjectBase::_check_template_params().
 *
 *	@date 2012-08-27
 *
 *	Successfuly tested under x64, fixed some compatibility issues.
 *
 *	@date 2012-09-11
 *
 *	The referencing allocator was abandonned in favor of Eigen::Map.
 *
 *	@date 2012-19-12
 *
 *	Resolved Linux compatibility problems, especially added small namespace
 *	fbs_ut and moved there fixed block size utility classes from CFixedBlockSize_Ops,
 *	not depending on CFixedBlockSize_Ops template parameters.
 *
 *	To avoid moving all the loops from CFixedBlockSize_Ops, a new template
 *	parameter _TyGppContext was added to all the loop templates, so that g++
 *	stops complaining about namespaces.
 *
 *	A serious bug in CDenseAllocator was fixed, where m_data_pool, a member
 *	of CUberBlockMatrix was accessed, instead of CDenseAllocator::m_r_pool.
 *	This would cause arithmetic operations allocate matrix blocks in a wrong
 *	(source - not destination, or vice versa) matrix, and in turn, memory
 *	access violations if the matrix was deleted.
 *
 *	@date 2012-10-24
 *
 *	Added the CUberBlockMatrix::m_n_ref_elem_num counter to be able to track
 *	data, referenced from other matrices.
 *
 *	@date 2012-10-26
 *
 *	Added many new functions CUberBlockMatrix::From_Sparse(),
 *	CUberBlockMatrix::From_Sparse32(), CUberBlockMatrix::p_Convert_to_Sparse_UpperTriangular()
 *	and CUberBlockMatrix::p_Convert_to_Sparse_UpperTriangular32(). These are
 *	mostly untested. Make unit tests for those.
 *
 *	Also implemented CUberBlockMatrix::SliceTo() and CUberBlockMatrix::PermuteTo(),
 *	these also require testing. CUberBlockMatrix::CopyTo() is now implemented
 *	as full slice, it should rather be implemented as another function.
 *
 *	@todo - perform the testing described above
 *
 *	@date 2012-11-17
 *
 *	Fixed a bug, affecting From_Sparse() and From_Sparse32() functions as well as their
 *	parallel variants. The bug was triggered if there was a column, which had nonzero elements
 *	in two block rows, with at least one completely zero block row in between. Matrices like
 *	these are quite scarce in SLAM, and were generated by using elementwise cholesky with
 *	elementwise AMD ordering. The bug caused the application to crash (out-of-bounds array access).
 *
 *	Provided dummy implementation of function CUberBlockMatrix::CopyLayoutTo() for testing
 *	(not a high performance implementation).
 *
 *	@todo - write unit tests to prevent such bugs in future
 *
 *	@date 2012-11-28
 *
 *	Fixed a bug in PreMultiplyWithSelfTransposeTo_FBS_Parallel() which used itself as a serial
 *	fallback in case OpenMP was not available, leading to stack overflow.
 *
 *	Added matrix value initialization to make debugging easier, now in case the
 *	__UBER_BLOCK_MATRIX_BUFFER_BOUNDS_CHECK macro is defined, the matrix storage
 *	is initialized upon allocating new blocks, and the integrity can be checked
 *	(avoids blocks overwriting data of other blocks in case the caller uses pointers
 *	to block data and mismatches the block size)
 *
 *	@date 2013-02-07
 *
 *	Cleaned up a lot of debugging code in order to remove clutter and to simplify.
 *
 *	@date 2013-07-16
 *
 *	Added block matrix I/O functions.
 *
 *	Placed (debug-only) integrity checks at the begginging of matrix ops, rather than at the end.
 *	While this causes catching malformed matrices later, it should prevent crashes and the
 *	annoying exponential cost of re-checking the matrix after adding every block is lifted.
 *
 *	Implemented copy-constructor and copy-operator.
 *
 *	@date 2013-12-05
 *
 *	Split this file to multiple files in order to reduce clutter, moved all inline functions
 *	longer than two lines to slam/BlockMatrix.inl. Moved all *_FBS() functions to
 *	slam/BlockMatrixFBS.inl.
 *
 *	Fixed intellisense (there was greater-than comparison as a template argument, which breaks it).
 *
 *	@date 2016-02-13
 *
 *	Renamed CUberBlockMatrix::b_BlockSquare() to b_Square_BlockSquare() since it checks the matrix
 *	for both having square dimensions and having the number of block rows equal to the number
 *	of block columns. The old name was easily misunderstood.
 *
 *	Added some new matrix property functions, namely CUberBlockMatrix::b_UpperBlockTriangular(),
 *	CUberBlockMatrix::b_StrictlyUpperBlockTriangular(), CUberBlockMatrix::b_LowerBlockTriangular(),
 *	CUberBlockMatrix::b_StrictlyLowerBlockTriangular(), CUberBlockMatrix::b_No_Diagonal_Blocks(),
 *	CUberBlockMatrix::b_All_Diagonal_Blocks(), CUberBlockMatrix::b_SymmetricBlockStructure() and
 *	CUberBlockMatrix::b_SymmetricBlocksOnly().
 *
 *	Added an implementation of CUberBlockMatrix::TriangularViewOf() which makes a shallow (optionally
 *	deep) copy of matrix triangles. Supports upper or lower triangle, including the diagonal.
 *
 *	@date 2016-08-05
 *
 *	Renamed CUberBlockMatrix::p_Convert_to_Sparse_UpperDiagonal_Debug() to the correct name
 *	p_Convert_to_Sparse_UpperTriangular_Debug(). Backwards compatibility was not retained in this
 *	case as it is a debugging function, presumably not used in production code.
 *
 */

/** \addtogroup ubm
 *	@{
 */

#include "slam/BlockMatrixBase.h"
#include <algorithm> // std::sort()
#include <numeric> // std::accumulate()

#if defined(_MSC_VER) && !defined(__MWERKS__)
#pragma warning(disable: 4227) // disable MSVC varning "anachronism used : qualifiers on reference are ignored"
// todo - compare timing in G++, see if it is worth it
#endif // _MSC_VER && !__MWERKS__

/**
 *	@brief the block matrix class
 */
class CUberBlockMatrix : public blockmatrix_detail::CUberBlockMatrix_Base {
public:
	/**
	 *	@brief constructor with structure specification
	 *
	 *	This takes arrays of cumsums - as many entries as blocks, the first is the width (height),
	 *	of the first block, the last is the width (height) of the matrix.
	 *
	 *	@tparam CIterator is iterator data type
	 *	@param[in] p_rows_cumsum_begin is iterator pointing to the first row cumulative sum
	 *	@param[in] p_rows_cumsum_end is iterator pointing one past the last row cumulative sum
	 *	@param[in] p_columns_cumsum_begin is iterator pointing to the first column cumulative sum
	 *	@param[in] p_columns_cumsum_end is iterator pointing one past the last column cumulative sum
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note It is often beneficial to only supply the row structure to avoid reindexing
	 *		and leave column structure empty.
	 */
	template <class CIterator>
	inline CUberBlockMatrix(CIterator p_rows_cumsum_begin, CIterator p_rows_cumsum_end,
		CIterator p_columns_cumsum_begin, CIterator p_columns_cumsum_end) // throw(std::bad_alloc)
		:blockmatrix_detail::CUberBlockMatrix_Base(p_rows_cumsum_begin, p_rows_cumsum_end,
		p_columns_cumsum_begin, p_columns_cumsum_end)
	{}

	/**
	 *	@brief constructor with structure specification
	 *
	 *	This takes arrays of cumsums - as many entries as blocks, the first is the height,
	 *	of the first block, the last is the height of the matrix.
	 *
	 *	@tparam CIterator is iterator data type
	 *	@param[in] p_rows_cumsum_begin is iterator pointing to the first row cumulative sum
	 *	@param[in] p_rows_cumsum_end is iterator pointing one past the last row cumulative sum
	 *	@param[in] n_column_block_num is number of column blocks (to avoid reallocation, can be
	 *		an estimate - the number of columns of the matrix is set to 0 regardless of the value)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This seems to be the fastest way in UFSMC benchmarks, even on "good" matrices
	 *		(matrices with no dummy rows).
	 */
	template <class CIterator>
	inline CUberBlockMatrix(CIterator p_rows_cumsum_begin,
		CIterator p_rows_cumsum_end, size_t n_column_block_num) // throw(std::bad_alloc)
		:blockmatrix_detail::CUberBlockMatrix_Base(p_rows_cumsum_begin, p_rows_cumsum_end, n_column_block_num)
	{}

	/**
	 *	@brief default constructor
	 *
	 *	@param[in] n_target_row_block_num is number of row blocks (to avoid reallocation, can be
	 *		an estimate - the number of rows of the matrix is set to 0 regardless of the value)
	 *	@param[in] n_target_column_block_num is number of column blocks (to avoid reallocation, can be
	 *		an estimate - the number of columns of the matrix is set to 0 regardless of the value)
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	inline CUberBlockMatrix(size_t n_target_row_block_num = 0,
		size_t n_target_column_block_num = 0) // throw(std::bad_alloc)
		:blockmatrix_detail::CUberBlockMatrix_Base(n_target_row_block_num, n_target_column_block_num)
	{}

	/**
	 *	@brief copy-constructor
	 *	@param[in] r_matrix is matrix to copy from
	 *	@note This function throws std::bad_alloc.
	 */
	inline CUberBlockMatrix(const CUberBlockMatrix &r_matrix)
		:blockmatrix_detail::CUberBlockMatrix_Base(r_matrix)
	{}

	/**
	 *	@brief copy-operator
	 *	@param[in] r_matrix is matrix to copy from
	 *	@return Returns reference to this.
	 *	@note This function throws std::bad_alloc.
	 */
	inline CUberBlockMatrix &operator =(const CUberBlockMatrix &r_matrix)
	{
		r_matrix.CopyTo(*this);
		return *this;
	}

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image
	 *
	 *	@param[in] p_storage is preallocated bitmap (can be 0; the pointer
	 *		to the structure is guaranteed not to change, the framebuffer pointer may change)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns pointer to the generated bitmap on success, 0 on failure.
	 */
	TBmp *p_Rasterize(TBmp *p_storage = 0, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image
	 *
	 *	This version rasterizes a triangularly represented matrix as a symmetric matrix,
	 *	without forming the transpose. This matrix must be square and have a symmetric layout.
	 *
	 *	@param[in] p_storage is preallocated bitmap (can be 0; the pointer
	 *		to the structure is guaranteed not to change, the framebuffer pointer may change)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns pointer to the generated bitmap on success, 0 on failure.
	 */
	TBmp *p_Rasterize_Symmetric(TBmp *p_storage = 0, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image
	 *
	 *	@param[in] r_prev_state is a copy of the previous state of the matrix for nonzero tracking
	 *		(its layout must be a prefix of this matrix layout, must be smaller or same size)
	 *	@param[in] b_handle_changed_as_new is new block handling flag
	 *	@param[in] p_storage is preallocated bitmap (can be 0; the pointer
	 *		to the structure is guaranteed not to change, the framebuffer pointer may change)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns pointer to the generated bitmap on success, 0 on failure.
	 */
	TBmp *p_Rasterize(const CUberBlockMatrix &r_prev_state, bool b_handle_changed_as_new,
		TBmp *p_storage = 0, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image and saves as .tga
	 *
	 *	@param[in] p_s_filename is output file name (.tga)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns true on success, false on failure.
	 */
	bool Rasterize(const char *p_s_filename, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image and saves as .tga
	 *
	 *	This version rasterizes a triangularly represented matrix as a symmetric matrix,
	 *	without forming the transpose. This matrix must be square and have a symmetric layout.
	 *
	 *	@param[in] p_s_filename is output file name (.tga)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns true on success, false on failure.
	 */
	bool Rasterize_Symmetric(const char *p_s_filename, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image and saves as .tga
	 *
	 *	This version rasterizes a triangularly represented matrix as a symmetric matrix,
	 *	without forming the transpose. This matrix must be square and have a symmetric layout.
	 *
	 *	@param[in] r_prev_state is a copy of the previous state of the matrix for nonzero tracking
	 *		(its layout must be a prefix of this matrix layout, must be smaller or same size)
	 *	@param[in] b_handle_changed_as_new is new block handling flag
	 *	@param[in] p_s_filename is output file name (.tga)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns true on success, false on failure.
	 */
	bool Rasterize_Symmetric(const CUberBlockMatrix &r_prev_state, bool b_handle_changed_as_new,
		const char *p_s_filename, int n_scalar_size = 5) const;

	/**
	 *	@brief debugging functionality; transforms matrix structure to an image and saves as .tga
	 *
	 *	@param[in] r_prev_state is a copy of the previous state of the matrix for nonzero tracking
	 *		(its layout must be a prefix of this matrix layout, must be smaller or same size)
	 *	@param[in] b_handle_changed_as_new is new block handling flag
	 *	@param[in] p_s_filename is output file name (.tga)
	 *	@param[in] n_scalar_size is size of one scalar, in pixels
	 *
	 *	@return Returns true on success, false on failure.
	 */
	bool Rasterize(const CUberBlockMatrix &r_prev_state, bool b_handle_changed_as_new,
		const char *p_s_filename, int n_scalar_size = 5) const;

	/**
	 *	@brief gets number of rows of a block
	 *	@param[in] n_row_index is (zero-based) index of a row (in blocks!)
	 *	@return Returns number of rows of the specified block row.
	 */
	inline size_t n_BlockRow_Row_Num(size_t n_row_index) const
	{
		_ASSERTE(n_row_index < m_block_rows_list.size());
		return m_block_rows_list[n_row_index].n_height;
	}

	/**
	 *	@brief gets number of columns of a block
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@return Returns number of columns of the specified block column.
	 */
	inline size_t n_BlockColumn_Column_Num(size_t n_column_index) const
	{
		_ASSERTE(n_column_index < m_block_cols_list.size());
		return m_block_cols_list[n_column_index].n_width;
	}

	/**
	 *	@brief gets starting row of a block
	 *	@param[in] n_row_index is (zero-based) index of a row (in blocks!)
	 *	@return Returns starting row of the specified block row.
	 */
	inline size_t n_BlockRow_Base(size_t n_row_index) const
	{
		_ASSERTE(n_row_index < m_block_rows_list.size());
		return m_block_rows_list[n_row_index].n_cumulative_height_sum;
	}

	/**
	 *	@brief gets starting column of a block
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@return Returns starting column of the specified block column.
	 */
	inline size_t n_BlockColumn_Base(size_t n_column_index) const
	{
		_ASSERTE(n_column_index < m_block_cols_list.size());
		return m_block_cols_list[n_column_index].n_cumulative_width_sum;
	}

	/**
	 *	@brief gets number of matrix rows
	 *	@return Returns number of matrix rows (in scalar elements!).
	 */
	inline size_t n_Row_Num() const;

	/**
	 *	@brief gets number of matrix columns
	 *	@return Returns number of matrix columns (in scalar elements!).
	 */
	inline size_t n_Column_Num() const;

	/**
	 *	@brief gets number of matrix rows blocks
	 *	@return Returns number of matrix rows blocks (in blocks!).
	 */
	inline size_t n_BlockRow_Num() const
	{
		return m_block_rows_list.size();
	}

	/**
	 *	@brief gets number of matrix columns blocks
	 *	@return Returns number of matrix columns blocks (in blocks!).
	 */
	inline size_t n_BlockColumn_Num() const
	{
		return m_block_cols_list.size();
	}

	/**
	 *	@brief gets number of blocks in a column
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *	@return Returns number of blocks in a specified column.
	 */
	inline size_t n_BlockColumn_Block_Num(size_t n_column_index) const
	{
		return m_block_cols_list[n_column_index].block_list.size();
	}

	/**
	 *	@brief gets row index of a certain block in a column
	 *
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@param[in] n_block_index is (zero-based) index of a block in the selected column
	 *
	 *	@return Returns row index of selected block in a specified column.
	 */
	inline size_t n_Block_Row(size_t n_column_index, size_t n_block_index) const
	{
		return m_block_cols_list[n_column_index].block_list[n_block_index].first;
	}

	/**
	 *	@brief tries to find a block row by first element offset, fails if it doesn't exist
	 *
	 *	@param[in] n_row is (zero-based) index of row (in elements)
	 *	@param[out] r_n_block_row_num is number of elements in the corresponding block-row
	 *
	 *	@return Returns (zero-based) index of block-row that contains
	 *		the given row on success, or -1 on failure.
	 */
	inline size_t n_Find_BlockRow(size_t n_row, size_t &r_n_block_row_num) const;

	/**
	 *	@brief tries to find a block column by first element offset, fails if it doesn't exist
	 *
	 *	@param[in] n_column is (zero-based) index of column (in elements)
	 *	@param[out] r_n_block_column_num is number of elements in the corresponding block-column
	 *
	 *	@return Returns (zero-based) index of block-column that contains
	 *		the given column on success, or -1 on failure.
	 */
	inline size_t n_Find_BlockColumn(size_t n_column, size_t &r_n_block_column_num) const;

	/**
	 *	@brief gets a matrix block (read-only)
	 *
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@param[in] n_block_index is (zero-based) index of a block (in blocks!)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question.
	 *
	 *	@note This *does not* perform row lookup, n_block_index is block index within
	 *		the column, and it depends on the actual fill of this column.
	 *	@note This function used to be called t_BlockAt() and the unfortunate order of arguments
	 *		was swapped, to match n_Block_Row() where column is first.
	 */
	inline _TyMatrixXdRef t_Block_AtColumn(size_t n_column_index, size_t n_block_index);

	/**
	 *	@brief gets a matrix block (read-only)
	 *
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@param[in] n_block_index is (zero-based) index of a block (in blocks!)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question.
	 *
	 *	@note This *does not* perform row lookup, n_block_index is block index within
	 *		the column, and it depends on the actual fill of this column.
	 *	@note This function used to be called t_BlockAt() and the unfortunate order of arguments
	 *		was swapped, to match n_Block_Row() where column is first.
	 */
	inline _TyConstMatrixXdRef t_Block_AtColumn(size_t n_column_index, size_t n_block_index) const;

	/**
	 *	@brief gets a fixed-size matrix block (read-only)
	 *
	 *	@tparam n_block_row_num is explected number of rows in the block
	 *	@tparam n_block_column_num is explected number of columns in the block
	 *
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@param[in] n_block_index is (zero-based) index of a block (in blocks!)
	 *
	 *	@return Returns a constant-sized Eigen::Map<> to the block in question.
	 *
	 *	@note This *does not* perform row lookup, n_block_index is block index within
	 *		the column, and it depends on the actual fill of this column.
	 *	@note This function used to be called t_BlockAt() and the unfortunate order of arguments
	 *		was swapped, to match n_Block_Row() where column is first.
	 */
	template <const int n_block_row_num, const int n_block_column_num>
	inline typename CMakeMatrixRef<n_block_row_num, n_block_column_num>::_TyConst
		t_Block_AtColumn(size_t n_column_index, size_t n_block_index) const;

	/**
	 *	@brief gets a fixed-size matrix block (read-only)
	 *
	 *	@tparam n_block_row_num is explected number of rows in the block
	 *	@tparam n_block_column_num is explected number of columns in the block
	 *
	 *	@param[in] n_column_index is (zero-based) index of a column (in blocks!)
	 *	@param[in] n_block_index is (zero-based) index of a block (in blocks!)
	 *
	 *	@return Returns a constant-sized Eigen::Map<> to the block in question.
	 *
	 *	@note This *does not* perform row lookup, n_block_index is block index within
	 *		the column, and it depends on the actual fill of this column.
	 *	@note This function used to be called t_BlockAt() and the unfortunate order of arguments
	 *		was swapped, to match n_Block_Row() where column is first.
	 */
	template <const int n_block_row_num, const int n_block_column_num>
	inline typename CMakeMatrixRef<n_block_row_num, n_block_column_num>::_Ty
		t_Block_AtColumn(size_t n_column_index, size_t n_block_index);

	/**
	 *	@brief gets number of allocated blocks
	 *	@return Returns number of allocated blocks.
	 *	@note This takes O(cols) time, where cols is number of matrix columns.
	 */
	inline size_t n_Block_Num() const;

	/**
	 *	@brief gets number of elements, allocated in storage
	 *	@return Returns number of elements, allocated in storage (in elements).
	 *	@note This takes O(1) time, and it can be used as approximation of n_Block_Num()
	 *		in matrices where all the blocks have the same size.
	 */
	inline size_t n_Storage_Size() const
	{
		return m_data_pool.size() + m_n_ref_elem_num;
	}

	/**
	 *	@brief gets number of elements, allocated in the storage of this matrix
	 *
	 *	@return Returns number of elements, allocated in storage (in elements).
	 *
	 *	@note This takes O(1) time, and it can be used as approximation of n_Block_Num()
	 *		in matrices where all the blocks have the same size.
	 *	@note This doesn't include elements in referenced storage, such as in
	 *		the matrices filled by SliceTo(), CopyTo() or PermuteTo() with the
	 *		data sharing flag set.
	 */
	inline size_t n_OwnStorage_Size() const
	{
		return m_data_pool.size();
	}

	/**
	 *	@brief gets number of elements, referenced from another matrix storage
	 *
	 *	This applies to matrices filled by SliceTo(), CopyTo() or PermuteTo()
	 *	with the data sharing flag set.
	 *
	 *	@return Returns number of elements, allocated in storage (in elements).
	 *	@note This takes O(1) time, and it can be used as approximation of n_Block_Num()
	 *		in matrices where all the blocks have the same size.
	 */
	inline size_t n_RefStorage_Size() const
	{
		return m_n_ref_elem_num;
	}

	/**
	 *	@brief calculates matrix norm
	 *	@return Returns L-inf norm of this matrix.
	 */
	double f_LInfNorm() const;

	/**
	 *	@brief calculates matrix norm
	 *	@return Returns L2 norm of this matrix.
	 */
	inline double f_Norm() const
	{
		return sqrt(f_Squared_Norm());
	}

	/**
	 *	@brief calculates squared matrix norm
	 *	@return Returns squared L2 norm of this matrix.
	 */
	double f_Squared_Norm() const;

	/**
	 *	@brief determines whether the matrix is square
	 *	@return Returns true if the matrix is square, false otherwise.
	 */
	inline bool b_Square() const
	{
		return m_n_row_num == m_n_col_num;
	}

	/**
	 *	@brief determines whether the matrix is empty
	 *	@return Returns true if the matrix is empty, false otherwise.
	 */
	inline bool b_Empty() const;

	/**
	 *	@brief determines whether the matrix is square, also counted by blocks
	 *	@return Returns true if the matrix is square, false otherwise.
	 *	@note The block square-ness can only be useful if it's known in advance that
	 *		all the block rows and block columns are not empty. If this doesn't naturally
	 *		apply to this matrix, use b_Square() instead.
	 */
	inline bool b_Square_BlockSquare() const
	{
		return b_Square() && m_block_rows_list.size() == m_block_cols_list.size();
	}

	/**
	 *	@brief determines whether the matrix has symmetric layout and nonzero pattern
	 *
	 *	@return Returns true if the matrix is structurally symmetric, otherwise returns false.
	 *
	 *	@note This only determines structural symmetry, the matrix can still be numerically asymmetric,
	 *	@note This function and throws std::bad_alloc (requires O(n) storage in the number of block columns).
	 */
	inline bool b_SymmetricBlockStructure() const // throw(std::bad_alloc)
	{
		return b_SymmetricLayout() && b_SymmetricBlocksOnly();
	}

	/**
	 *	@brief determines whether the matrix has only blocks with symmetric
	 *		counterparts on the other side of the block diagonal
	 *
	 *	@return Returns false if the matrix has some nonzero block <tt>(i, j)</tt> but
	 *		does not have <tt>(j, i)</tt>, otherwise returns true.
	 *
	 *	@note This only determines structural symmetry, the matrix can still be numerically asymmetric,
	 *	@note This does not imply the matrix being square, block square or having symmetric layout.
	 *	@note The diagonal blocks are ignored in this function.
	 *	@note This function and throws std::bad_alloc (requires O(n) storage in the number of block columns).
	 */
	bool b_SymmetricBlocksOnly() const; // throw(std::bad_alloc)

	/**
	 *	@brief determines whether the matrix has off-diagonal blocks
	 *
	 *	@return Returns true if the matrix has any has off-diagonal blocks, otherwise returns false.
	 *
	 *	@note This does not imply symmetric layout.
	 *	@note See also b_BlockDiagonal(). Note that this function is not a simple negation of it.
	 */
	bool b_OffDiagonal_Blocks() const;

	/**
	 *	@brief determines whether the matrix has no diagonal blocks
	 *
	 *	@return Returns true if the matrix has any has no blocks at the diagonal, otherwise returns false.
	 *
	 *	@note This does not imply symmetric layout.
	 *	@note This function only considers the upper/left square portion of the matrix in
	 *		case it is rectangular (does not change the semantics of this function though).
	 *	@note See also b_AllDiagonal_Blocks(). Note that this function is not a simple negation of it.
	 */
	bool b_No_Diagonal_Blocks() const;

	/**
	 *	@brief determines whether the matrix has all of the diagonal blocks
	 *
	 *	@return Returns true if the matrix has any has no blocks at the diagonal, otherwise returns false.
	 *
	 *	@note This does not imply symmetric layout.
	 *	@note This does not imply the matrix being block diagonal; the presence of other off-diagonal blocks
	 *		does not change the result either way.
	 *	@note This function only considers the upper/left square portion of the matrix in case it is rectangular.
	 *	@note See also b_No_Diagonal_Blocks(). Note that this function is not a simple negation of it.
	 */
	bool b_All_Diagonal_Blocks() const;

	/**
	 *	@brief determines whether the matrix is symmetric and has block diagonal
	 *
	 *	@return Returns true if the matrix is symmetric and has all the blocks of
	 *		the main diagonal and no blocks anywhere else, otherwise returns false.
	 *
	 *	@note This returns false if the layout is not symmetric (but the matrix may
	 *		still have blocks only on the diagonal). This also returns false if the
	 *		matrix is rank deficient (some blocks of the diagonal are missing).
	 *	@note See also b_OffDiagonalBlocks(). Note that this function is not
	 *		a simple negation of it.
	 */
	bool b_BlockDiagonal() const;

	/**
	 *	@brief determines whether the matrix is upper block triangular
	 *
	 *	@return Returns true if the matrix has no blocks in the lower triangle,
	 *		otherwise returns false.
	 *
	 *	@note A matrix with no off-diagonal blocks (diagonal, partial diagonal or empty)
	 *		is also upper (and at the same time lower) triangular in this sense. Use in
	 *		conjunction with \ref b_OffDiagonal_Blocks() if needed.
	 *	@note This does *not* imply symmetric layout, full structural rank
	 *		or the matrix being square.
	 */
	bool b_UpperBlockTriangular() const;

	/**
	 *	@brief determines whether the matrix is strictly upper block triangular
	 *
	 *	@return Returns true if the matrix has no blocks in the lower triangle
	 *		or on the diagonal, otherwise returns false.
	 *
	 *	@note An empty matrix is also strictly upper (and at the same time lower)
	 *		triangular in this sense. Use in conjunction with
	 *		\ref b_OffDiagonal_Blocks() if needed.
	 *	@note This does *not* imply symmetric layout, full structural rank
	 *		or the matrix being square.
	 */
	bool b_StrictlyUpperBlockTriangular() const;

	/**
	 *	@brief determines whether the matrix is lower block triangular
	 *
	 *	@return Returns true if the matrix has no blocks in the upper triangle,
	 *		otherwise returns false.
	 *
	 *	@note A matrix with no off-diagonal blocks (diagonal, partial diagonal or empty)
	 *		is also lower (and at the same time upper) triangular in this sense. Use in
	 *		conjunction with \ref b_OffDiagonal_Blocks() if needed.
	 *	@note This does *not* imply symmetric layout, full structural rank
	 *		or the matrix being square.
	 */
	bool b_LowerBlockTriangular() const;

	/**
	 *	@brief determines whether the matrix is strictly lower block triangular
	 *
	 *	@return Returns true if the matrix has no blocks in the upper triangle
	 *		or on the diagonal, otherwise returns false.
	 *
	 *	@note An empty matrix is also strictly lower (and at the same time upper)
	 *		triangular in this sense. Use in conjunction with
	 *		\ref b_OffDiagonal_Blocks() if needed.
	 *	@note This does *not* imply symmetric layout, full structural rank
	 *		or the matrix being square.
	 */
	bool b_StrictlyLowerBlockTriangular() const;

	/**
	 *	@brief determines whether the matrix is upper triangular (elementwise)
	 *
	 *	@return Returns true if the matrix has no nonzeros in the (strictly) lower triangle,
	 *		otherwise returns false.
	 *
	 *	@note This needs to visit elements of all the blocks which touch the diagonal
	 *		or are below it. The worst-case complexity is O(nnz).
	 *	@note A matrix with no off-diagonal nonzeros (elementwise diagonal, partial
	 *		elementwise diagonal or zero) is also upper (and at the same time lower)
	 *		triangular in this sense.
	 *	@note This does *not* imply there are no blocks under the diagonal (they just
	 *		need to be all zero).
	 */
	bool b_UpperTriangular() const;

	/**
	 *	@brief determines whether the matrix is lower triangular (elementwise)
	 *
	 *	@return Returns true if the matrix has no nonzeros in the (strictly) upper triangle,
	 *		otherwise returns false.
	 *
	 *	@note This needs to visit elements of all the blocks which touch the diagonal
	 *		or are above it. The worst-case complexity is O(nnz).
	 *	@note A matrix with no off-diagonal nonzeros (elementwise diagonal, partial
	 *		elementwise diagonal or zero) is also lower (and at the same time upper)
	 *		triangular in this sense.
	 *	@note This does *not* imply there are no blocks above the diagonal (they just
	 *		need to be all zero).
	 */
	bool b_LowerTriangular() const;

	/**
	 *	@brief determines whether the matrix has symmetric layout
	 *	@return Returns true if the matrix has symmetric row and column layout, false otherwise.
	 *	@note Despite the layout being symmetric, the blocks might be present at places
	 *		that are not at diagonal, or opposite, resulting in the matrix not being symmetric.
	 *		See \ref b_SymmetricBlockStructure() to get structural symmetry.
	 */
	bool b_SymmetricLayout() const;

	/**
	 *	@brief compares block layout of two matrices
	 *
	 *	@param[in] r_other is the other matrix to compare to
	 *
	 *	@return Returns true if the matrices have the same column and row layout
	 *		(but ignores the number and positions of nonzero blocks and their data),
	 *		otherwise returns false.
	 *
	 *	@note This compares structure, not including nonzero block layout (without it,
	 *		it's hard to tell because of the possibility of splitting empty rows / columns);
	 *		hard to say if it's actually useful for anything.
	 */
	bool b_EqualLayout(const CUberBlockMatrix &r_other) const;

	/**
	 *	@brief compares structure of two matrices
	 *
	 *	@param[in] r_other is the other matrix to compare to
	 *
	 *	@return Returns true if the matrices have the same column and row layout
	 *		and the same number and positions of nonzero blocks (but ignores data),
	 *		otherwise returns false.
	 *
	 *	@note This compares structure including nonzero block layout (without it,
	 *		it's hard to tell because of the possibility of splitting empty rows / columns);
	 *		hard to say if it's actually useful for anything.
	 *	@note Nope, it is very useful for debugging incremental nonlinear solvers where
	 *		the matrices are first allocated, but the content is not there yet (just
	 *		uninitialized bloks).
	 */
	bool b_EqualStructure(const CUberBlockMatrix &r_other) const;

	/**
	 *	@brief compares two matrices
	 *	@param[in] r_other is the other matrix to compare to
	 *	@return Returns true if the matrices have the same column and row layout
	 *		and the same number, positions and values of nonzero blocks,
	 *		otherwise returns false.
	 *	@note This is only suitable for methods using identical calculus,
	 *		otherwise use b_Equal(const CUberBlockMatrix&, double).
	 */
	bool b_Equal(const CUberBlockMatrix &r_other) const;

	/**
	 *	@brief compares two matrices with tolerance
	 *
	 *	@param[in] r_other is the other matrix to compare to
	 *	@param[in] f_tolerance is maximum allowed absolute difference
	 *
	 *	@return Returns true if the matrices have the same column and row layout
	 *		and the same number, positions and values of nonzero blocks,
	 *		otherwise returns false.
	 */
	bool b_Equal(const CUberBlockMatrix &r_other, double f_tolerance) const;

	/**
	 *	@brief gets the number of non-zero elements
	 *
	 *	@return Returns number of non-zero elements.
	 *
	 *	@note This is calculated by elements (rather than by blocks).
	 *	@note This takes O(blocks) time, where blocks is number of allocated blocks.
	 *	@note This just merely counts blocks area, it ignores the fact that
	 *		some of the block elements may, in fact, be zero.
	 */
	size_t n_NonZero_Num() const;

	/**
	 *	@brief gets the number of non-zero elements in a symmetric matrix
	 *		which only stores upper-triangular elements
	 *
	 *	This function takes care of not counting the diagonal twice.
	 *
	 *	@return Returns number of non-zero elements.
	 *
	 *	@note This is calculated by elements (rather than by blocks).
	 *	@note This takes O(blocks) time, where blocks is number of allocated blocks.
	 *	@note This just merely counts blocks area, it ignores the fact that
	 *		some of the block elements may, in fact, be zero.
	 */
	size_t n_Symmetric_NonZero_Num() const;

	/**
	 *	@brief gets the ratio of the non-zero elements to the size of the matrix (the sparsity)
	 *
	 *	@return Returns the ratio of the non-zero elements to the size of the matrix (a number
	 *		in the [0, 1] interval, 0 being a null matrix and 1 being fully dense).
	 *
	 *	@note This is calculated by elements (rather than by blocks).
	 *	@note This takes O(blocks) time, where blocks is number of allocated blocks.
	 *	@note This just merely counts blocks area, it ignores the fact that
	 *		some of the block elements may, in fact, be zero.
	 */
	inline double f_NonZero_Ratio() const
	{
		if(!m_n_col_num || m_n_row_num <= SIZE_MAX / m_n_col_num)
			return double(n_NonZero_Num()) / (m_n_row_num * m_n_col_num); // the area multiplication does not overflow
		else
			return (double(n_NonZero_Num()) / m_n_row_num) / m_n_col_num; // slightly less precise
	}

	/**
	 *	@brief gets the ratio of the non-zero elements to the size of the matrix(the sparsity;
	 *		version for symmetric matrices which only store the upper-triangular elements)
	 *
	 *	@return Returns the ratio of the non-zero elements to the size of the matrix (a number
	 *		in the [0, 1] interval, 0 being a null matrix and 1 being fully dense).
	 *
	 *	@note This is calculated by elements (rather than by blocks).
	 *	@note This takes O(blocks) time, where blocks is number of allocated blocks.
	 *	@note This just merely counts blocks area, it ignores the fact that
	 *		some of the block elements may, in fact, be zero.
	 */
	inline double f_Symmetric_NonZero_Ratio() const
	{
		if(!m_n_col_num || m_n_row_num <= SIZE_MAX / m_n_col_num)
			return double(n_Symmetric_NonZero_Num()) / (m_n_row_num * m_n_col_num); // the area multiplication does not overflow
		else
			return (double(n_Symmetric_NonZero_Num()) / m_n_row_num) / m_n_col_num; // slightly less precise
	}

	/**
	 *	@brief extends the matrix to a given size by adding empty block row / column at the bottom / right
	 *
	 *	@param[in] n_row_num is number of rows (must be greater or equal
	 *		to the current number of rows; in elements)
	 *	@param[in] n_column_num is number of columns (must be greater or equal
	 *		to the current number of columns; in elements)
	 *
	 *	@note The empty block row / column is *always* added, even if the right-most
	 *		column / bottom-most row are empty. Some implementation already relies
	 *		on this behavior, won't change it.
	 *	@note This function throws std::bad_alloc.
	 */
	void ExtendTo(size_t n_row_num, size_t n_column_num); // throw(std::bad_alloc)

	/**
	 *	@brief extends the matrix to a given size by adding empty block row / column at the top / left
	 *
	 *	Adding a new row is a potentially costy operation as all the block row indices need to be
	 *	increased, leading to O(n) in the number of blocks rather than O(1) as might be expected.
	 *
	 *	@param[in] n_row_num is number of rows (must be greater or equal
	 *		to the current number of rows; in elements)
	 *	@param[in] n_column_num is number of columns (must be greater or equal
	 *		to the current number of columns; in elements)
	 *
	 *	@note The empty block row / column is *always* added, even if the right-most
	 *		column / bottom-most row are empty. Some implementation already relies
	 *		on this behavior, won't change it.
	 *	@note This function throws std::bad_alloc.
	 */
	void ExtendTopLeftTo(size_t n_row_num, size_t n_column_num); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[in] b_alloc_if_not_found is block allocation flag (if set and
	 *		the specified block doesn't exist, it is allocated; if not set,
	 *		the function fails; note that the function can still fail if block
	 *		position or dimensions violate block layout even though the flag is set)
	 *	@param[in] b_mind_uninitialized is the initialization flag (if set,
	 *		the functions clears the block data to zero if the block didn't exist)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	inline _TyMatrixXdRef t_FindBlock(size_t n_row, size_t n_column,
		size_t n_block_row_num, size_t n_block_column_num, bool b_alloc_if_not_found = true,
		bool b_mind_uninitialized = true); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 */
	inline _TyMatrixXdRef t_FindBlock(size_t n_row, size_t n_column);

	/**
	 *	@brief finds a matrix block (read-only)
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *
	 *	@return Returns a const Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 */
	inline _TyConstMatrixXdRef t_FindBlock(size_t n_row, size_t n_column) const;

	/**
	 *	@brief finds a matrix block, strictly inside the matrix
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[in] b_alloc_if_not_found is block allocation flag (if set and
	 *		the specified block doesn't exist, it is allocated; if not set,
	 *		the function fails; note that the function can still fail if block
	 *		position or dimensions violate block layout even though the flag is set)
	 *	@param[in] b_mind_uninitialized is the initialization flag (if set,
	 *		the functions clears the block data to zero if the block didn't exist)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This can only allocate new block rows / columns at the edge of the matrix.
	 */
	_TyMatrixXdRef t_GetBlock_Log(size_t n_row_index, size_t n_column_index,
		size_t n_block_row_num, size_t n_block_column_num, bool b_alloc_if_not_found = true,
		bool b_mind_uninitialized = true); // throw(std::bad_alloc)

	/**
	 *	@brief gets a matrix block
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *
	 *	@return Returns an Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 */
	inline _TyMatrixXdRef t_GetBlock_Log(size_t n_row_index, size_t n_column_index);

	/**
	 *	@brief gets a matrix block (read-only)
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *
	 *	@return Returns a const Eigen::Map<> to the block in question on success,
	 *		or an empty 0 x 0 matrix on failure.
	 */
	inline _TyConstMatrixXdRef t_GetBlock_Log(size_t n_row_index, size_t n_column_index) const;

	/**
	 *	@brief finds a matrix block (read-only)
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 */
	inline const double *p_GetBlock_Log(size_t n_row_index, size_t n_column_index,
		size_t n_block_row_num, size_t n_block_column_num) const;

	/**
	 *	@brief finds a matrix block, strictly inside the matrix
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[in] b_alloc_if_not_found is block allocation flag (if set and
	 *		the specified block doesn't exist, it is allocated; if not set,
	 *		the function fails; note that the function can still fail if block
	 *		position or dimensions violate block layout even though the flag is set)
	 *	@param[in] b_mind_uninitialized is the initialization flag (if set,
	 *		the functions clears the block data to zero if the block didn't exist)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This can only allocate new block rows / columns at the edge of the matrix.
	 */
	double *p_GetBlock_Log(size_t n_row_index, size_t n_column_index,
		size_t n_block_row_num, size_t n_block_column_num, bool b_alloc_if_not_found /*= true*/, // otherwise ambiguous
		bool b_mind_uninitialized = true); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block, strictly inside the matrix, or allocates it
	 *
	 *	@param[in] n_row_index is index of a row (in blocks!)
	 *	@param[in] n_column_index is index of a column (in blocks!)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[out] r_b_is_uninitialized is the initialization flag
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This can only allocate new block rows / columns at the edge of the matrix.
	 */
	double *p_GetBlock_Log_Alloc(size_t n_row_index, size_t n_column_index,
		size_t n_block_row_num, size_t n_block_column_num, bool &r_b_is_uninitialized); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[in] b_alloc_if_not_found is block allocation flag (if set and
	 *		the specified block doesn't exist, it is allocated; if not set,
	 *		the function fails; note that the function can still fail if block
	 *		position or dimensions violate block layout even though the flag is set)
	 *	@param[in] b_mind_uninitialized is the initialization flag (if set,
	 *		the functions clears the block data to zero if the block didn't exist)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	double *p_FindBlock(size_t n_row, size_t n_column, size_t n_block_row_num,
		size_t n_block_column_num, bool b_alloc_if_not_found /*= true*/, // otherwise ambiguous
		bool b_mind_uninitialized = true); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block, or allocates it
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *	@param[out] r_b_is_uninitialized is the initialization flag
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	double *p_FindBlock_Alloc(size_t n_row, size_t n_column, size_t n_block_row_num,
		size_t n_block_column_num, bool &r_b_is_uninitialized); // throw(std::bad_alloc)

	/**
	 *	@brief finds a matrix block
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 */
	double *p_FindBlock(size_t n_row, size_t n_column,
		size_t n_block_row_num, size_t n_block_column_num);

	/**
	 *	@brief finds a matrix block (read-only)
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[in] n_block_row_num is number of block rows (in elements)
	 *	@param[in] n_block_column_num is number of block columns (in elements)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 */
	const double *p_FindBlock(size_t n_row, size_t n_column,
		size_t n_block_row_num, size_t n_block_column_num) const;

	/**
	 *	@brief finds a matrix block (read-only)
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[out] r_n_block_row_num is filled with number of block rows (in elements)
	 *	@param[out] r_n_block_column_num is filled with number of block columns (in elements)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 */
	double *p_FindBlock_ResolveSize(size_t n_row, size_t n_column,
		size_t &r_n_block_row_num, size_t &r_n_block_column_num);

	/**
	 *	@brief finds a matrix block (read-only)
	 *
	 *	@param[in] n_row is starting row of the block (in elements)
	 *	@param[in] n_column is starting column of the block (in elements)
	 *	@param[out] r_n_block_row_num is filled with number of block rows (in elements)
	 *	@param[out] r_n_block_column_num is filled with number of block columns (in elements)
	 *
	 *	@return Returns pointer to raw data (storage order same as in Eigen) on success,
	 *		or 0 on failure.
	 */
	const double *p_FindBlock_ResolveSize(size_t n_row, size_t n_column,
		size_t &r_n_block_row_num, size_t &r_n_block_column_num) const;

	/**
	 *	@brief appends a new block to the matrix, changes matrix dimensions, if required
	 *
	 *	@tparam CMatrixType is dense matrix type (e.g. Eigen::MatrixXd)
	 *
	 *	@param[in] r_t_block is a dense block to be appended
	 *	@param[in] n_row is row position (in elements) where the new block should be inserted
	 *	@param[in] n_column is column position (in elements) where the new block should be inserted
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function can be used to overwrite existing blocks.
	 *	@note This function throws std::bad_alloc.
	 *	@note This is slightly slower than calling Append_Block_Log().
	 */
	template <class CMatrixType>
	bool Append_Block(const CMatrixType &r_t_block, size_t n_row, size_t n_column); // throw(std::bad_alloc)

	/**
	 *	@brief appends a new block to the matrix, changes matrix dimensions, if required
	 *
	 *	@tparam n_compile_time_row_num number of rows
	 *	@tparam n_compile_time_col_num number of columns
	 *	@tparam n_options a combination of either Eigen::RowMajor or Eigen::ColMajor,
	 *		and of either Eigen::AutoAlign or Eigen::DontAlign
	 *
	 *	@param[in] r_t_block is a dense block to be appended
	 *	@param[in] n_row is row position (in elements) where the new block should be inserted
	 *	@param[in] n_column is column position (in elements) where the new block should be inserted
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function can be used to overwrite existing blocks.
	 *	@note This function throws std::bad_alloc.
	 *	@note This is slightly slower than calling Append_Block_Log().
	 */
	template <int n_compile_time_row_num, int n_compile_time_col_num, int n_options>
	bool Append_Block(const Eigen::Matrix<double, n_compile_time_row_num, n_compile_time_col_num,
		n_options, n_compile_time_row_num, n_compile_time_col_num> &r_t_block, size_t n_row, size_t n_column); // throw(std::bad_alloc)

	/**
	 *	@brief appends a new block to the matrix, changes matrix dimensions, if required
	 *
	 *	@tparam CMatrixType is dense matrix type (e.g. Eigen::MatrixXd)
	 *
	 *	This allows for direct specification of block row / block column. The indices
	 *	may actually specify a new block row / block column, if equal to the return value of
	 *	n_BlockRow_Num() / n_BlockColumn_Num(), respectively.
	 *
	 *	@param[in] r_t_block is a dense block to be appended
	 *	@param[in] n_row_index is index of a row (in blocks!) where the new block should be inserted
	 *	@param[in] n_column_index is index of a column (in blocks!) where the new block should be inserted
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This is slightly faster than Append_Block().
	 */
	template <class CMatrixType>
	bool Append_Block_Log(const CMatrixType &r_t_block, size_t n_row_index, size_t n_column_index); // throw(std::bad_alloc)

	/**
	 *	@brief appends a new block to the matrix, changes matrix dimensions, if required
	 *
	 *	@tparam n_compile_time_row_num number of rows
	 *	@tparam n_compile_time_col_num number of columns
	 *	@tparam n_options a combination of either Eigen::RowMajor or Eigen::ColMajor,
	 *		and of either Eigen::AutoAlign or Eigen::DontAlign
	 *
	 *	This allows for direct specification of block row / block column. The indices
	 *	may actually specify a new block row / block column, if equal to the return value of
	 *	n_BlockRow_Num() / n_BlockColumn_Num(), respectively.
	 *
	 *	@param[in] r_t_block is a dense block to be appended
	 *	@param[in] n_row_index is index of a row (in blocks!) where the new block should be inserted
	 *	@param[in] n_column_index is index of a column (in blocks!) where the new block should be inserted
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This is slightly faster than Append_Block().
	 */
	template <int n_compile_time_row_num, int n_compile_time_col_num, int n_options>
	bool Append_Block_Log(const Eigen::Matrix<double, n_compile_time_row_num, n_compile_time_col_num,
		n_options, n_compile_time_row_num, n_compile_time_col_num> &r_t_block, size_t n_row_index, size_t n_column_index); // throw(std::bad_alloc)

	/**
	 *	@brief accumulates multiplication of vector by matrix (the matrix is on the left)
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector (for debugging purposes only; must equal matrix rows)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector (for debugging purposes only; must equal matrix columns)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + this \cdot p\_src\_vector\f$. To calculate the product
	 *		in reverse order, use PostMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 */
	void PreMultiply_Add(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief accumulates multiplication of vector by matrix (the matrix is on the left)
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector
	 *		(for debugging purposes only; must equal matrix rows)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector
	 *		in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector
	 *		(for debugging purposes only; must equal matrix columns)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + this \cdot p\_src\_vector\f$. To calculate the product
	 *		in reverse order, use PostMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 */
	template <class CBlockMatrixTypelist>
	void PreMultiply_Add_FBS(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief accumulates multiplication of matrix by vector (the matrix is on the right)
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector (for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector (for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$. To calculate the product
	 *		in reverse order, use PreMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 *	@note This function is easily parallelized using OpenMP (use PostMultiply_Add_Parallel()).
	 *	@note It is possible to use the following relationship to convert premultiply to postmultiply: \f$M^T \cdot u = (u \cdot M)^T\f$.
	 */
	void PostMultiply_Add(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

	/**
	 *	@brief accumulates multiplication of matrix by vector (the matrix is on the right)
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector (for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector (for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$. To calculate the product
	 *		in reverse order, use PreMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 *	@note This function is parallelized using OpenMP (if not available, it runs serially).
	 *	@note It is possible to use the following relationship to convert premultiply to postmultiply: \f$M^T \cdot u = (u \cdot M)^T\f$.
	 */
	void PostMultiply_Add_Parallel(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief accumulates multiplication of matrix by vector (the matrix is on the right)
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector
	 *		(for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector
	 *		in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector
	 *		(for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$. To calculate the product
	 *		in reverse order, use PreMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 *	@note This function is easily parallelized using OpenMP (use PostMultiply_Add_FBS_Parallel()).
	 *	@note It is possible to use the following relationship to convert premultiply
	 *		to postmultiply: \f$M^T \cdot u = (u \cdot M)^T\f$.
	 */
	template <class CBlockMatrixTypelist>
	void PostMultiply_Add_FBS(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

	/**
	 *	@brief accumulates multiplication of matrix by vector (the matrix is on the right)
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector
	 *		(for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector
	 *		in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector
	 *		(for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$. To calculate the product
	 *		in reverse order, use PreMultiply_Add().
	 *	@note Error-checking is enabled in debug only.
	 *	@note This function is parallelized using OpenMP (if not available, runs in serial).
	 *	@note It is possible to use the following relationship to convert premultiply
	 *		to postmultiply: \f$M^T \cdot u = (u \cdot M)^T\f$.
	 */
	template <class CBlockMatrixTypelist>
	void PostMultiply_Add_FBS_Parallel(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size)) const;

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief calculates multiplication of a symmetric matrix by vector
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector
	 *		(for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector
	 *		in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector
	 *		(for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$
	 *		which is the same as \f$p\_dest\_vector + (this \cdot p\_src\_vector^T)^T\f$. The order
	 *		does not matter.
	 *	@note This requires the matrix to have symmetric layout.
	 */
	void SymmetricMultiply_Add(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size), bool b_use_upper_triangle = true) const;

	/**
	 *	@brief calculates multiplication of a symmetric matrix by vector, with explicitly
	 *		using only one triangle of the diagonal blocks (upper or lower, based on the
	 *		value of the last argument)
	 *
	 *	@param[out] p_dest_vector is destination vector, must be allocated, and
	 *		if only the multiplication is required, it must also be cleared
	 *	@param[in] n_dest_size is number of elements in p_dest_vector
	 *		(for debugging purposes only; must equal matrix columns)
	 *	@param[in] p_src_vector is source vector (must not equal p_dest_vector
	 *		in this implementation)
	 *	@param[in] n_src_size is number of elements in p_src_vector
	 *		(for debugging purposes only; must equal matrix rows)
	 *
	 *	@note This calculates \f$p\_dest\_vector = p\_dest\_vector + p\_src\_vector \cdot this\f$
	 *		which is the same as \f$p\_dest\_vector + (this \cdot p\_src\_vector^T)^T\f$. The order
	 *		does not matter.
	 *	@note This requires the matrix to have symmetric layout.
	 */
	void SymmetricMultiply_Add_ExpDiag(double *p_dest_vector, size_t UNUSED(n_dest_size),
		const double *p_src_vector, size_t UNUSED(n_src_size), bool b_use_upper_triangle = true) const;

	/**
	 *	@brief calculates numbers of nonzeros in each column of \f$A+A^T\f$ where A is block structure of this matrix
	 *
	 *	@tparam b_upper_triangular is upper-triangular input flag (the presence of the diagonal
	 *		does not matter either way; the function will fail if the input is not upper triangular)
	 *	@tparam b_likely_upper_triangular is likely upper-triangular input flag (mutually exclusive
	 *		with b_upper_triangular; the function will be slower but will not fail on non-upper input)
	 *	@tparam b_output_diagonal is diagonal output flag (if set, the block diagonal structure of
	 *		this matrix will be present in the output; otherwise only the off-diagonal blocks are processed)
	 *	@tparam b_output_full_diagonal is diagonal output flag (mutually exclusive with
	 *		b_output_diagonal; if set, the output will always contain a full diagonal spanning the
	 *		entire \f$A+A^T\f$)
	 *	@tparam CInt is integer data type for the column lengths array (can be signed or unsigned)
	 *	@tparam CInt1 is integer data type for the workspace array (can be signed or unsigned)
	 *
	 *	@param[out] p_column_lengths is pointer to the array to store the numbers of nonzeros in each
	 *		column of \f$A+A^T\f$ (must be allocated to the greater dimension of this matrix, in blocks)
	 *	@param[in] n_length_num is size of the p_column_lengths array (must match the greater dimension
	 *		of this matrix, in blocks)
	 *	@param[out] p_workspace is pointer to a temporary array (must be allocated at least to the
	 *		number of block columns of this matrix unless b_upper_triangular is set; may not reuse
	 *		the same array as p_column_lengths)
	 *	@param[in] n_workspace_size is size of the p_workspace array (must be at least the number
	 *		of block columns of this matrix unless b_upper_triangular is set)
	 *
	 *	@return Returns the number of nonzero elements in \f$A+A^T\f$, possibly with the diagonal
	 *		omitted (or extended) based on the template arguments.
	 *
	 *	@note In case b_upper_triangular is set, the workspace array is not needed.
	 *	@note This is fast, allocation-free function. For convenience, use
	 *		\ref p_BlockStructure_SumWithSelfTransposeNoDiag_to_Sparse().
	 */
	template <bool b_upper_triangular, bool b_likely_upper_triangular,
		bool b_output_diagonal, bool b_output_full_diagonal, class CInt, class CInt1>
	size_t n_BlockStructure_SumWithSelfTranspose_ColumnLengths(CInt *__restrict p_column_lengths,
		size_t n_length_num, CInt1 *__restrict p_workspace, size_t n_workspace_size) const;

	/**
	 *	@brief calculates binary pattern of \f$A+A^T\f$ where A is block structure of this matrix
	 *
	 *	@tparam b_upper_triangular is upper-triangular input flag (the presence of the diagonal
	 *		does not matter either way; the function will fail if the input is not upper triangular)
	 *	@tparam b_likely_upper_triangular is likely upper-triangular input flag (mutually exclusive
	 *		with b_upper_triangular; the function will be slower but will not fail on non-upper input)
	 *	@tparam b_output_diagonal is diagonal output flag (if set, the block diagonal structure of
	 *		this matrix will be present in the output; otherwise only the off-diagonal blocks are processed)
	 *	@tparam b_output_full_diagonal is diagonal output flag (mutually exclusive with
	 *		b_output_diagonal; if set, the output will always contain a full diagonal spanning the
	 *		entire \f$A+A^T\f$)
	 *	@tparam b_need_sorted_items is sorted output flag (if set, the row indices will be strictly
	 *		ordered in each column; if not set, they may come in arbitrary order unless the input
	 *		matrix is either symmetric or triangular; note that e.g. AMD does not require sorted rows)
	 *	@tparam CInt is integer data type for the compressed column pointers array (can be signed or unsigned)
	 *	@tparam CInt1 is integer data type for the column lengths array (can be signed or unsigned)
	 *	@tparam CInt2 is integer data type for the workspace array (can be signed or unsigned)
	 *
	 *	@param[out] p_column_ptrs is pointer to the array to be filled with compressed column pointers
	 *		(must be allocated to one plus the greater dimension of this matrix, in blocks)
	 *	@param[in] n_column_ptr_num is size of the p_column_ptrs array (must equal one plus the greater
	 *		dimension of this matrix, in blocks)
	 *	@param[out] p_row_inds is pointer to the array to be filled with row indices of nonzero entries
	 *		in \f$A+A^T\f$ (must be allocated to the total number of nonzeros, as computed e.g. by using
	 *		\ref n_BlockStructure_SumWithSelfTranspose_ColumnLengths())
	 *	@param[in] n_nnz_num is size of the p_row_inds array (must match the number of nonzeros of \f$A+A^T\f$,
	 *		as returned by \ref n_BlockStructure_SumWithSelfTranspose_ColumnLengths(), which also equals
	 *		the sum of all p_column_lengths elements)
	 *	@param[in] p_column_lengths is pointer to the array with the numbers of nonzeros in each
	 *		column of \f$A+A^T\f$ (as computed e.g. using \ref n_BlockStructure_SumWithSelfTranspose_ColumnLengths())
	 *	@param[in] n_length_num is size of the p_column_lengths array (must match the greater dimension
	 *		of this matrix, in blocks)
	 *	@param[out] p_workspace is pointer to a temporary array (must be allocated at least to the
	 *		number of block columns of this matrix unless b_upper_triangular is set; may not reuse
	 *		the same array as p_column_lengths)
	 *	@param[in] n_workspace_size is size of the p_workspace array (must be at least the number
	 *		of block columns of this matrix unless b_upper_triangular is set)
	 *
	 *	@note In case b_upper_triangular is set, the workspace array is not needed.
	 *	@note This is fast, allocation-free function. For convenience, use
	 *		\ref p_BlockStructure_SumWithSelfTransposeNoDiag_to_Sparse().
	 */
	template <bool b_upper_triangular, bool b_likely_upper_triangular, bool b_output_diagonal,
		bool b_output_full_diagonal, bool b_need_sorted_items, class CInt, class CInt1, class CInt2>
	void BlockStructure_SumWithSelfTranspose(CInt *p_column_ptrs, size_t n_column_ptr_num,
		CInt *__restrict p_row_inds, size_t n_nnz_num, const CInt1 *__restrict p_column_lengths,
		size_t n_length_num, CInt2 *p_workspace, size_t n_workspace_size) const;

#ifdef __UBER_BLOCK_MATRIX_HAVE_CSPARSE

	/**
	 *	@brief converts block structure to a binary sparse matrix
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note The resulting matrix is binary, and unless p_alloc contains array for values,
	 *		no element values are generated.
	 *
	 *	@todo Document what happens to the original matrix if it fails (and elsewhere).
	 */
	cs *p_BlockStructure_to_Sparse(cs *p_alloc = 0) const;

	/**
	 *	@brief calculates a sum of block structure of this matrix with the block
	 *		structure of its transpose while omitting a diagonal (a binary sparse matrix)
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note The row indices of the output matrix are always sorted (not jumbled).
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note The resulting matrix is binary, and unless p_alloc contains array for values,
	 *		no element values are generated.
	 *	@note This function throws std::bad_alloc. In case it throws, p_alloc is not modified.
	 */
	cs *p_BlockStructure_SumWithSelfTransposeNoDiag_to_Sparse(cs *p_alloc = 0) const; // throw(std::bad_alloc)

	/**
	 *	@brief converts block structure to a binary sparse matrix, with mini-skirt
	 *
	 *	This is an optimization for incremental reordering. This can convert only
	 *	a (lower-right) part of the matrix, and can replace a (upper-left) part
	 *	of it by a diagonal matrix (a sparse apron - the "mini-skirt"). For example,
	 *	on the matrix with the following nonzero block structure:
	 *
	 *	@code
	 *	+                 +
	 *	|_*|____*|________|
	 *	|  |*   *|*       |
	 *	|  |  * *|        |
	 *	|__|____*|__*_*___|
	 *	|  |     |*     * |
	 *	|  |     |  *   * |
	 *	|  |     |    * * |
	 *	|  |     |      * |
	 *	+  |     |        +
	 *	   |     |
	 *	   |     +-> n_diag_block_num
	 *	   +-------> n_min_block
	 *	@endcode
	 *
	 *	With n_min_block = 1 and n_diag_block_num = 4, it will produce a sparse
	 *	elementwise binary matrix:
	 *
	 *	@code
	 *	+               +
	 *	| 1    |1       |
	 *	|   1  |        |
	 *	|_____1|__1_1___|
	 *	|      |1     1 |
	 *	|      |  1   1 |
	 *	|      |    1 1 |
	 *	|      |      1 |
	 *	+               +
	 *	@endcode
	 *
	 *	Hence, the first row and column are skipped (n_min_block = 1),
	 *	and the next 3 rows and columns are replaced by a diagonal matrix
	 *	(n_diag_block_num - n_min_block = 3).
	 *
	 *	@param[in] n_min_block is (zero-based) index of the first block
	 *		row and column to be converted
	 *	@param[in] n_diag_block_num is (zero-based) index of the one
	 *		past last block row and column that will be replaced with unit diagonal
	 *		(counted from the beginning of the matrix, not from n_min_block)
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note The resulting matrix is binary, and unless p_alloc contains array for values,
	 *		no element values are generated.
	 *
	 *	@todo Document what happens to the original matrix if it fails (and elsewhere).
	 */
	cs *p_BlockStructure_to_Sparse_Apron(size_t n_min_block,
		size_t n_diag_block_num, cs *p_alloc = 0) const;

#if defined(_M_X64) || defined(_M_AMD64) || defined(_M_IA64) || defined(__x86_64) || defined(__amd64) || defined(__ia64)

	/**
	 *	@brief converts block structure to a binary sparse matrix,
	 *		forces 32-bit integers in the cs structure
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note This is potentially dangerous as it forces 32-bit integers in the cs structure,
	 *		which may define integers as 64-bit and expect them to be so. Use with caution.
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note The resulting matrix is binary, and unless p_alloc contains array for values,
	 *		no element values are generated.
	 */
	cs *p_BlockStructure_to_Sparse32(cs *p_alloc = 0) const;

	/**
	 *	@brief calculates a sum of block structure of this matrix with the block
	 *		structure of its transpose while omitting a diagonal (a binary sparse matrix),
	 *		forces 32-bit integers in the cs structure
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note This is potentially dangerous as it forces 32-bit integers in the cs structure,
	 *		which may define integers as 64-bit and expect them to be so. Use with caution.
	 *	@note The row indices of the output matrix are always sorted (not jumbled).
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note The resulting matrix is binary, and unless p_alloc contains array for values,
	 *		no element values are generated.
	 *	@note This function throws std::bad_alloc. In case it throws, p_alloc is not modified.
	 */
	cs *p_BlockStructure_SumWithSelfTransposeNoDiag_to_Sparse32(cs *p_alloc = 0) const; // throw(std::bad_alloc)

#endif // _M_X64 || _M_AMD64 || _M_IA64 || __x86_64 || __amd64 || __ia64

	/**
	 *	@brief converts to sparse matrix
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note This uses data pool for estimating the number of nonzero elements, with
	 *		addition to m_n_ref_elem_num, it will now work even with shallow copied matrices,
	 *		made using CopyTo() or SliceTo().
	 */
	cs *p_Convert_to_Sparse(cs *p_alloc = 0) const;

#if defined(_M_X64) || defined(_M_AMD64) || defined(_M_IA64) || defined(__x86_64) || defined(__amd64) || defined(__ia64)

	/**
	 *	@brief converts to sparse matrix, forces 32-bit integers in the cs structure
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note This is potentially dangerous as it forces 32-bit integers in the cs structure,
	 *		which may define integers as 64-bit and expect them to be so. Use with caution.
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note This uses data pool for estimating the number of nonzero elements, with
	 *		addition to m_n_ref_elem_num, it will now work even with shallow copied matrices,
	 *		made using CopyTo() or SliceTo().
	 */
	cs *p_Convert_to_Sparse32(cs *p_alloc = 0) const;

#endif // _M_X64 || _M_AMD64 || _M_IA64 || __x86_64 || __amd64 || __ia64

	/**
	 *	@brief converts to sparse, strictly upper triangular matrix
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note This uses data pool for estimating the number of nonzero elements, with
	 *		addition to m_n_ref_elem_num, it will now work even with shallow copied matrices,
	 *		made using CopyTo() or SliceTo().
	 */
	cs *p_Convert_to_Sparse_UpperTriangular(cs *p_alloc = 0) const;

#if defined(_M_X64) || defined(_M_AMD64) || defined(_M_IA64) || defined(__x86_64) || defined(__amd64) || defined(__ia64)

	/**
	 *	@brief converts to sparse, strictly upper triangular matrix,
	 *		forces 32-bit integers in the cs structure
	 *
	 *	@param[in] p_alloc is pointer to before-allocated sparse matrix
	 *		(contents will be overwritten; can be null)
	 *
	 *	@return Returns pointer to sparse column-compressed matrix (equal to p_alloc
	 *		if supplied) on success, 0 on failure (not enough memory).
	 *
	 *	@note This is potentially dangerous as it forces 32-bit integers in the cs structure,
	 *		which may define integers as 64-bit and expect them to be so. Use with caution.
	 *	@note If p_alloc is supplied, the pointer to the matrix structure is guaranteed
	 *		to not change. However, the internal pointers in it may change.
	 *	@note If p_alloc is supplied, it can be allocated to any number of nonzero elements.
	 *		If needed, more space is added automatically.
	 *	@note This uses data pool for estimating the number of nonzero elements, with
	 *		addition to m_n_ref_elem_num, it will now work even with shallow copied matrices,
	 *		made using CopyTo() or SliceTo().
	 */
	cs *p_Convert_to_Sparse_UpperTriangular32(cs *p_alloc = 0) const;

#endif // _M_X64 || _M_AMD64 || _M_IA64 || __x86_64 || __amd64 || __ia64

	/**
	 *	@brief fills matrix contents from sparse matrix (does not initialize matrix structure)
	 *
	 *	@param[in] n_base_row_id is (zero-based) index of the first
	 *		block row where the sparse data should be placed
	 *	@param[in] n_base_column_id is (zero-based) index of the
	 *		first block column where the sparse data should be placed
	 *	@param[in] p_sparse is the sparse matrix to be filled from
	 *		(there must be enough space in this between the selected
	 *		row and column and the lower-right corner of the matrix
	 *		to fit the sparse matrix in)
	 *	@param[in] b_transpose is sparse matrix transpose flag (must be false)
	 *	@param[in] r_workspace is workspace for fast row id lookup
	 *		(must be either empty, or contain results for (a lower part)
	 *		of the same block matrix as this)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This does not erase the matrix at the beginning. Any existing contents will
	 *		remain there, if not overwritten by the new data.
	 */
	bool From_Sparse(size_t n_base_row_id, size_t n_base_column_id,
		const cs *p_sparse, bool b_transpose, std::vector<size_t> &r_workspace); // throw(std::bad_alloc)

	/**
	 *	@copydoc From_Sparse()
	 *	@note This is parallelized using OpenMP. If not available, runs in serial.
	 */
	bool From_Sparse_Parallel(size_t n_base_row_id, size_t n_base_column_id,
		const cs *p_sparse, bool b_transpose, std::vector<size_t> &r_workspace); // throw(std::bad_alloc)

#if defined(_M_X64) || defined(_M_AMD64) || defined(_M_IA64) || defined(__x86_64) || defined(__amd64) || defined(__ia64)

	/**
	 *	@brief fills matrix contents from sparse matrix, expects 32-bit integers
	 *		in the cs structure (does not initialize matrix structure)
	 *
	 *	@param[in] n_base_row_id is (zero-based) index of the first
	 *		block row where the sparse data should be placed
	 *	@param[in] n_base_column_id is (zero-based) index of the
	 *		first block column where the sparse data should be placed
	 *	@param[in] p_sparse is the sparse matrix to be filled from
	 *		(there must be enough space in this between the selected
	 *		row and column and the lower-right corner of the matrix
	 *		to fit the sparse matrix in)
	 *	@param[in] b_transpose is sparse matrix transpose flag (must be false)
	 *	@param[in] r_workspace is workspace for fast row id lookup
	 *		(must be either empty, or contain results for (a lower part)
	 *		of the same block matrix as this)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This is potentially dangerous as it expects 32-bit integers in the cs structure,
	 *		which may define integers as 64-bit and expect them to be so. Use with caution.
	 *	@note This does not erase the matrix at the beginning. Any existing contents will
	 *		remain there, if not overwritten by the new data.
	 */
	bool From_Sparse32(size_t n_base_row_id, size_t n_base_column_id,
		const cs *p_sparse, bool b_transpose, std::vector<size_t> &r_workspace); // throw(std::bad_alloc)

	/**
	 *	@copydoc From_Sparse32()
	 *	@note This is parallelized using OpenMP. If not available, runs in serial.
	 */
	bool From_Sparse32_Parallel(size_t n_base_row_id, size_t n_base_column_id,
		const cs *p_sparse, bool b_transpose, std::vector<size_t> &r_workspace); // throw(std::bad_alloc)

#endif // _M_X64 || _M_AMD64 || _M_IA64 || __x86_64 || __amd64 || __ia64

	/**
	 *	@brief converts to sparse matrix using intermediate triplet matrix
	 *		(slower but easy to debug; kept for reference purposes)
	 *	@return Returns pointer to sparse column-compressed matrix on success,
	 *		or 0 on failure (not enough memory).
	 */
	cs *p_Convert_to_Sparse_Debug() const;

	/**
	 *	@brief converts the upper-triangular part to sparse matrix using intermediate
	 *		triplet matrix (slower but easy to debug; kept for reference purposes)
	 *	@return Returns pointer to sparse column-compressed matrix on success,
	 *		or 0 on failure (not enough memory).
	 */
	cs *p_Convert_to_Sparse_UpperTriangular_Debug() const;

#endif // __UBER_BLOCK_MATRIX_HAVE_CSPARSE

	/**
	 *	@brief copies the matrix to a part of another matrix
	 *
	 *	This function takes one matrix, and puts it on a selected position in another matrix.
	 *	Any blocks the destination matrix has in the area are overwritten (not an additive
	 *	operation). The layout of the destination area of the destination matrix must be the same
	 *	as the layout of this matrix (the layout is not modified in the operation).
	 *
	 *	@param[out] r_dest is destination matrix (need to have the same layout)
	 *	@param[in] n_dest_row_index is zero-based index of the first block row where to put this matrix
	 *	@param[in] n_dest_column_index is zero-based index of the first block column where to put this matrix
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	void PasteTo(CUberBlockMatrix &r_dest,
		size_t n_dest_row_index, size_t n_dest_column_index); // throw(std::bad_alloc)

	/**
	 *	@brief copies only the matrix layout, not the data
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *
	 *	@note This function throws std::bad_alloc.
	 *
	 *	@todo Make a version with slice? permutation?
	 */
	void CopyLayoutTo(CUberBlockMatrix &r_dest) const; // throw(std::bad_alloc)

	/**
	 *	@brief deletes all the blocks in the matrix, but keeps the layout
	 *
	 *	@note This does not specify size or layout of the matrix;
	 *		use an appropriate constructor, or CopyLayoutTo().
	 *	@note To delete also the layout, use Clear().
	 *	@note This deletes any prior contents of the matrix,
	 *		all pointers to the block data are invalidated.
	 *	@note In case this matrix is referencing another matrix, the changes will
	 *		not show in the referenced matrix. Use Scale() with factor zero instead.
	 */
	void SetZero();

	/**
	 *	@brief creates an identity matrix (must have symmetric layout)
	 *
	 *	@note This does not specify size or layout of the matrix;
	 *		use an appropriate constructor, or CopyLayoutTo().
	 *	@note This function throws std::bad_alloc.
	 *	@note This deletes any prior contents of the matrix,
	 *		all pointers to the block data are invalidated.
	 *	@note In case this matrix is referencing another matrix,
	 *		the changes will not show in the referenced matrix.
	 */
	void SetIdentity(); // throw(std::bad_alloc)

	/**
	 *	@brief slices the matrix
	 *
	 *	This creates a copy of an upper-left rectangular area in this matrix. This function
	 *	can work inplace: in that case, if b_share_data is true, the function attempts to
	 *	delete unused blocks from memory (note that it heavily depends on the order in which
	 *	the blocks were inserted (allocated)). It can leave dead memory in the matrix pool,
	 *	which can be deleted by slicing to a different matrix and then swapping the matrices.
	 *	But for well-ordered matrices (such as those produced with CholeskyOf()) there is
	 *	no dead memory left.
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten, can equal to this)
	 *	@param[in] n_block_row_num is number of block rows to keep (in blocks)
	 *	@param[in] n_block_column_num is number of block columns to keep (in blocks)
	 *	@param[in] b_share_data is data sharing flag (if set, block data in the destination
	 *		matrix will point to this matrix pool; if new blocks are added to the copy
	 *		or the original(this), these are not mirrored in the other matrix)
	 *
	 *	@note This function throws std::bad_alloc.
	 *
	 *	@todo This is largerly untested; test this.
	 */
	void SliceTo(CUberBlockMatrix &r_dest, size_t n_block_row_num,
		size_t n_block_column_num, bool b_share_data = false); // throw(std::bad_alloc)

	/**
	 *	@brief slices the matrix
	 *
	 *	This creates copy of a rectangular area in this matrix.
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *	@param[in] n_first_row_index is index of the first row block (in blocks)
	 *	@param[in] n_last_row_index is index of one past the last row block (in blocks)
	 *	@param[in] n_first_column_index is index of the first column block (in blocks)
	 *	@param[in] n_last_column_index is index of one past the last column block (in blocks)
	 *	@param[in] b_share_data is data sharing flag (if set, block data in the destination
	 *		matrix will point to this matrix pool; if new blocks are added to the copy
	 *		or the original(this), these are not mirrored in the other matrix)
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	void SliceTo(CUberBlockMatrix &r_dest, size_t n_first_row_index, size_t n_last_row_index,
		size_t n_first_column_index, size_t n_last_column_index, bool b_share_data = false) const; // throw(std::bad_alloc)

	/**
	 *	@brief applies blockwise symbolic permutation to a general matrix
	 *
	 *	@param[out] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] p_block_ordering is pointer to the block ordering vector
	 *	@param[in] n_ordering_size is size of the ordering (must match the number
	 *		of block columns of this matrix)
	 *	@param[in] b_reorder_rows is row reordering flag (if set, the rows are reordered,
	 *		if not set the rows are kept with identity ordering)
	 *	@param[in] b_reorder_columns is column reordering flag (if set, the columns are
	 *		reordered, if not set the columns are kept with identity ordering)
	 *	@param[in] b_share_data is data sharing flag; if not set a deep copy is made,
	 *		if set, a shallow copy is made and (all of) the blocks reference the data
	 *		of this matrix
	 *
	 *	@note This function throws std::bad_alloc.
	 *
	 *	@note This is a version for full / symmetric matrices, it will not work on triangular
	 *		matrices (will scatter data to the other diagonal half as well). See Permute_UpperTriangular_To().
	 */
	void PermuteTo(CUberBlockMatrix &r_dest, const size_t *p_block_ordering,
		size_t UNUSED(n_ordering_size), bool b_reorder_rows = true,
		bool b_reorder_columns = true, bool b_share_data = false) const; // throw(std::bad_alloc)

	/**
	 *	@brief applies blockwise symbolic permutation to an upper triangular matrix
	 *
	 *	@param[out] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] p_block_ordering is pointer to the block ordering vector
	 *	@param[in] n_ordering_size is size of the ordering (must match the number
	 *		of block columns of this matrix, or it can be shorter but then the
	 *		dest matrix will be smaller and number of referenced elements will not be precise)
	 *	@param[in] b_share_data is data sharing flag; if not set a deep copy is made,
	 *		if set, a shallow copy is made and the blocks reference the data of this
	 *		matrix, except if the blocks end up below the diagonal (due to permutation)
	 *		and need to be transposed - for these blocks the storage is allocated locally
	 *
	 *	@note This matrix must be square, symmetric and upper triangular.
	 *	@note This function throws std::bad_alloc.
	 *
	 *	@todo This is a version for upper diagonal matrices, it is unable to efficiently
	 *		share data (sometimes it needs to transpose and copy), make an incremental version
	 *		(data changed / the original matrix grew while the permutation only extended
	 *		and did not change).
	 */
	void Permute_UpperTriangular_To(CUberBlockMatrix &r_dest, const size_t *p_block_ordering,
		size_t UNUSED(n_ordering_size), bool b_share_data = false) const; // throw(std::bad_alloc)

	/**
	 *	@brief creates a (block) triangular view of another matrix
	 *
	 *	Use as follows:
	 *	@code
	 *	CUberBlockMatrix M = ...;
	 *
	 *	bool b_shallow_copy = true; // or false
	 *
	 *	CUberBlockMatrix upper; upper.TriangularViewOf(M, true, b_shallow_copy);
	 *
	 *	CUberBlockMatrix strictly_upper; strictly_upper.TriangularViewOf(M, true, b_shallow_copy, -1);
	 *	// shift the diagonal one block up
	 *
	 *	CUberBlockMatrix upper_Hessenberg; upper_Hessenberg.TriangularViewOf(M, true, b_shallow_copy, +1);
	 *	// shift the diagonal one block down to get a Hessenberg-like matrix
	 *
	 *	CUberBlockMatrix lower; lower.TriangularViewOf(M, false, b_shallow_copy);
	 *
	 *	CUberBlockMatrix strictly_lower; strictly_lower.TriangularViewOf(M, false, b_shallow_copy, +1);
	 *	// shift the diagonal one block down
	 *
	 *	CUberBlockMatrix lower_Hessenberg; lower_Hessenberg.TriangularViewOf(M, false, b_shallow_copy, -1);
	 *	// shift the diagonal one block up to get a Hessenberg-like matrix
	 *
	 *	CUberBlockMatrix tridiagonal; tridiagonal.TriangularViewOf(lower_Hessenberg, true, b_shallow_copy, +1);
	 *	// get a view of only the three diagonals of M
	 *	@endcode
	 *
	 *	@param[in] r_src is matrix to create a view of (must be square, with symmetric layout;
	 *		this does not work inplace)
	 *	@param[in] b_upper_triangular is upper triangular flag (if set, the upper triangle
	 *		is viewed; if cleared, the lower triangle is viewed)
	 *	@param[in] b_share_data is data sharing flag (if set, a shallow copy is made (default);
	 *		if cleared, the data is copied and the input matrix can be changed or deleted)
	 *	@param[in] n_block_diag_offset is diagonal offset in blocks (default 0; +1 selects one
	 *		diagonal below the main one, -1 selects one above the main one, greater offsets
	 *		are also possible)
	 *
	 *	@note Rectangular matrices are permitted, matrices with unsymmetric layout
	 *		will be block triangular but the blocks may not be aligned with the main
	 *		diagonal (layout symmetry is not checked here).
	 *	@note This function throws std::bad_alloc.
	 */
	void TriangularViewOf(const CUberBlockMatrix &r_src, bool b_upper_triangular,
		bool b_share_data = true, int n_block_diag_offset = 0); // throw(std::bad_alloc)

	/**
	 *	@brief views the matrix with a tolerance on the individual blocks
	 *
	 *	@param[in] r_src is matrix to create a view of
	 *	@param[in] f_tol is tolerance (default zero)
	 *	@param[in] b_use_abs_max is infinity norm flag (if set, absolute maximum is used,
	 *		otherwise L2 norm is used)
	 *	@param[in] b_share_data is data sharing flag (if set, a shallow copy is made (default);
	 *		if cleared, the data is copied and the input matrix can be changed or deleted)
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	void ToleranceViewOf(const CUberBlockMatrix &r_src, double f_tol = 0,
		bool b_use_abs_max = true, bool b_share_data = true); // throw(std::bad_alloc)

	/**
	 *	@brief creates a view of a full matrix from another (triangular) matrix
	 *
	 *	@param[in] r_src is matrix to create a view of (must be square, with symmetric layout)
	 *	@param[in] b_upper_triangular is upper triangular flag (if set, the upper triangle
	 *		is transposed to lower; if cleared, the lower triangle is transposed to upper)
	 *	@param[in] b_share_data is data sharing flag (if set, a shallow copy is made (default);
	 *		if cleared, the data is copied and the input matrix can be changed or deleted)
	 *
	 *	@note Since this is a real matrix, this is a self-transpose rather than self-adjoint
	 *		but that term is rarely used in the literature.
	 *	@note This assumes the diagonal blocks to be symmetric and leaves them intact.
	 *		In case they are not symmetric, the resulting matrix will not be symmetric
	 *		either. To guarantee symmetry, use \ref SelfAdjointView_ExpDiag_Of().
	 *	@note This can also work inplace but the matrix already needs to be triangular.
	 *	@note This function throws std::bad_alloc.
	 */
	void SelfAdjointViewOf(const CUberBlockMatrix &r_src, bool b_upper_triangular,
		bool b_share_data = true); // throw(std::bad_alloc)

	/**
	 *	@brief creates a view of a full matrix from another (triangular) matrix while
	 *		explicitly making self-adjoint views of the diagonal blocks
	 *
	 *	@param[in] r_src is matrix to create a view of (must be square, with symmetric layout)
	 *	@param[in] b_upper_triangular is upper triangular flag (if set, the upper triangle
	 *		is transposed to lower; if cleared, the lower triangle is transposed to upper)
	 *	@param[in] b_share_data is data sharing flag (if set, a shallow copy is made (default);
	 *		if cleared, the data is copied and the input matrix can be changed or deleted)
	 *
	 *	@note Since this is a real matrix, this is a self-transpose rather than self-adjoint
	 *		but that term is rarely used in the literature.
	 *	@note This assumes the diagonal blocks to be assymmetric and uses
	 *		\ref Eigen::MatrixBase::selfadjointView() to explicitly make them symmetric.
	 *		In case they are already symmetric, then this just runs slower. If speed is
	 *		required, use \ref SelfAdjointViewOf().
	 *	@note This can also work inplace but the matrix already needs to be triangular.
	 *	@note This function throws std::bad_alloc.
	 */
	void SelfAdjointView_ExpDiag_Of(const CUberBlockMatrix &r_src, bool b_upper_triangular,
		bool b_share_data = true); // throw(std::bad_alloc)

	/**
	 *	@brief applies blockwise symbolic permutation to an upper triangular matrix
	 *		with an option to only write to a lower right submatrix
	 *
	 *	@param[out] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] p_block_ordering is pointer to the block ordering vector
	 *	@param[in] n_ordering_size is size of the ordering (must match the number
	 *		of block columns of this matrix)
	 *	@param[in] b_share_data is data sharing flag; if not set a deep copy is made,
	 *		if set, a shallow copy is made and the blocks reference the data of this
	 *		matrix, except if the blocks end up below the diagonal (due to permutation)
	 *		and need to be transposed - for these blocks the storage is allocated locally
	 *	@param[in] n_min_block_row_column is zero-based index of minimal block row and
	 *		block column to populate with blocks (the lower parts are kept null)
	 *	@param[in] b_keep_upper_left_part is incremental reperm flag (if set, the part
	 *		of the destination matrix, before n_min_block_row_column is kept intact)
	 *
	 *	@note This matrix must be square, symmetric and upper triangular.
	 *	@note This function throws std::bad_alloc.
	 *
	 *	@todo This is largerly untested - test it.
	 */
	void Permute_UpperTriangular_To(CUberBlockMatrix &r_dest, const size_t *p_block_ordering,
		size_t UNUSED(n_ordering_size), bool b_share_data, size_t n_min_block_row_column,
		bool b_keep_upper_left_part = false) const; // throw(std::bad_alloc)

	/**
	 *	@brief transposes the matrix
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *	@note This function throws std::bad_alloc.
	 */
	void TransposeTo(CUberBlockMatrix &r_dest) const; // throw(std::bad_alloc)

	/**
	 *	@brief transposes the matrix (this will be overwritten)
	 *	@param[in] r_src is source matrix to be transposed
	 *	@note This function throws std::bad_alloc.
	 */
	inline void TransposeOf(const CUberBlockMatrix &r_src) // throw(std::bad_alloc)
	{
		r_src.TransposeTo(*this);
	}

	/**
	 *	@brief copies data from other matrix, overwrites existing blocks
	 *
	 *	@param[in] n_base_row_id is (zero-based) index of the first
	 *		block row where the sparse data should be placed
	 *	@param[in] n_base_column_id is (zero-based) index of the
	 *		first block column where the sparse data should be placed
	 *	@param[in] r_matrix is the block matrix to be filled from
	 *	@param[in] b_allow_layout_extension is layout extension flag
	 *		(if set and r_matrix reaches outside this matrix layout, the layout
	 *		is extended, if not set and this happens, the function fails)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@todo this seems to work, but it is largerly untested. test this.
	 *	@todo b_allow_layout_extension must currently be false, it is not implemented
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note In case b_allow_layout_extension is not set, the layout needs to be
	 *		exactly the same, subdivision of empty rows / columns will not be performed
	 *		even if it's needed (speed optimization).
	 */
	bool From_Matrix(size_t n_base_row_id, size_t n_base_column_id,
		const CUberBlockMatrix &r_matrix, bool b_allow_layout_extension = false); // throw(std::bad_alloc)

	/**
	 *	@brief gets position of the lowest-row block in every column in an upper-triangular matrix
	 *
	 *	The frontline of the matrix is an useful tool. It can be observed that Cholesky leaves nonzero
	 *	values in columns above the diagonal and under the top-most block of the original matrix
	 *	(the frontline).
	 *
	 *	@param[out] r_frontline is written the same amount of (zero-based) block row indices
	 *		as there are block columns in this matrix
	 *
	 *	@note In case the matrix contains empty columns, a nonzero diagonal block is assumed.
	 *	@note This function throws std::bad_alloc.
	 */
	void Get_UpperTriangular_BlockFrontline(std::vector<size_t> &r_frontline) const; // throw(std::bad_alloc)

	/**
	 *	@brief gets position of the lowest-row block in a specified range of columns
	 *
	 *	@param[in] n_order_lo is zero-based index of the first block column
	 *	@param[in] n_order_hi is zero-based index of one past the last block column
	 *
	 *	@return Returns minimum (zero-based) block row index,
	 *		occuring in the selected range of columns.
	 *
	 *	@note In case the matrix contains empty columns, a nonzero diagonal block is assumed.
	 */
	size_t n_Get_UpperTriangular_BlockFrontline_Minimum(size_t n_order_lo, size_t n_order_hi) const;

	/**
	 *	@brief gets the diagonal of this matrix
	 *
	 *	@tparam CEigenMatrixType is Eigen vector type for storing the diagonal
	 *
	 *	@param[out] r_dest is the destination vector (will be overwritten, does not need to be allocated)
	 *	@param[in] b_is_upper_triangular is the upper-triangular flag (if set, the matrix is expected
	 *		to be upper-triangular - the last block in each column is the diagonal one)
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	// *	@tparam Derived0 is Eigen derived matrix type for the first matrix argument
	template <class CEigenMatrixType/*Derived0*/> // Eigen::MatrixBase doesn't allow .resize()
	void Get_Diagonal(/*Eigen::MatrixBase<Derived0>*/CEigenMatrixType &r_dest,
		bool b_is_upper_triangular = false) const; // throw(std::bad_alloc)

	/**
	 *	@brief converts this matrix to Eigen dense matrix
	 *	@tparam Derived0 is Eigen derived matrix type for the first matrix argument
	 *	@param[out] r_dest is the destination matrix (allocated by the caller
	 *		(because Eigen::MatrixBase does not allow resize), will be overwritten)
	 *	@note This function throws std::bad_alloc.
	 */
	template <class Derived0>
	void Convert_to_Dense(Eigen::MatrixBase<Derived0> &r_dest) const; // throw(std::bad_alloc)

	/**
	 *	@brief converts this matrix to Eigen dense matrix
	 *	@param[out] r_dest is the destination matrix (allocated inside the function, will be overwritten)
	 *	@note This function throws std::bad_alloc.
	 */
	void Convert_to_Dense(Eigen::MatrixXd &r_dest) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs element-wise multiplication of this matrix by a scalar
	 *
	 *	\f$this = this \cdot f\_scalar\f$
	 *
	 *	@param[in] f_scalar is the multiplication factor
	 */
	inline void Scale(double f_scalar)
	{
		ElementwiseUnaryOp_ZeroInvariant(CScaleBy(f_scalar));
	}

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise multiplication of this matrix by a scalar
	 *
	 *	\f$this = this \cdot f\_scalar\f$
	 *
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *	@param[in] f_scalar is the multiplication factor
	 */
	template <class CBlockMatrixTypelist>
	inline void Scale_FBS(double f_scalar);

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise multiplication of this matrix by a scalar
	 *
	 *	\f$this = this \cdot f\_scalar\f$
	 *
	 *	@param[in] f_scalar is the multiplication factor
	 *	@note This version is parallelized using OpenMP (where not available,
	 *		the serial version is used).
	 */
	inline void Scale_Parallel(double f_scalar)
	{
		ElementwiseUnaryOp_ZeroInvariant_Parallel(CScaleBy(f_scalar));
	}

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise multiplication of this matrix by a scalar
	 *
	 *	\f$this = this \cdot f\_scalar\f$
	 *
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *	@param[in] f_scalar is the multiplication factor
	 *	@note This version is parallelized using OpenMP (where not available,
	 *		the serial version is used).
	 */
	template <class CBlockMatrixTypelist>
	inline void Scale_FBS_Parallel(double f_scalar);

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief unary operation on each block elements
	 *	@tparam COp is a function (can be e.g. sqrt()), or an function object that implements
	 *		double operator ()(double f_value) where value is somehow modified
	 *	@param[in] op is instance of COp
	 *	@note The zero-invariant requrement dicates that COp must hold op(0) == 0
	 *		(otherwise the structure of the matrix would change to dense matrix).
	 */
	template <class COp>
	void ElementwiseUnaryOp_ZeroInvariant(COp op);

	/**
	 *	@brief unary operation on each block elements
	 *
	 *	@tparam COp is a function (can be e.g. sqrt()), or an function object that implements
	 *		double operator ()(double f_value) where value is somehow modified
	 *
	 *	@param[in] op is instance of COp
	 *
	 *	@note The zero-invariant requrement dicates that COp must hold op(0) == 0
	 *		(otherwise the structure of the matrix would change to dense matrix).
	 *	@note This version is parallelized using OpenMP (where not available,
	 *		the serial version is used).
	 */
	template <class COp>
	void ElementwiseUnaryOp_ZeroInvariant_Parallel(COp op);

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief unary operation on each block elements
	 *
	 *	@tparam COp is a function (can be e.g. sqrt()), or an function object that implements
	 *		double operator ()(double f_value) where value is somehow modified
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[in] op is instance of COp
	 *
	 *	@note The zero-invariant requrement dicates that COp must hold op(0) == 0
	 *		(otherwise the structure of the matrix would change to dense matrix).
	 */
	template <class CBlockMatrixTypelist, class COp>
	void ElementwiseUnaryOp_ZeroInvariant_FBS(COp op);

	/**
	 *	@brief unary operation on each block elements
	 *
	 *	@tparam COp is a function (can be e.g. sqrt()), or an function object that implements
	 *		double operator ()(double f_value) where value is somehow modified
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[in] op is instance of COp
	 *
	 *	@note The zero-invariant requrement dicates that COp must hold op(0) == 0
	 *		(otherwise the structure of the matrix would change to dense matrix).
	 *	@note This version is parallelized using OpenMP (where not available,
	 *		the serial version is used).
	 */
	template <class CBlockMatrixTypelist, class COp>
	void ElementwiseUnaryOp_ZeroInvariant_FBS_Parallel(COp op);

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices
	 *
	 *	\f$r\_dest = r\_dest + this\f$
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *	@note This function throws std::bad_alloc.
	 */
	inline bool AddTo(CUberBlockMatrix &r_dest) const // throw(std::bad_alloc)
	{
		return ElementwiseBinaryOp_ZeroInvariant(r_dest, f_Add);
		// t_odo - optimize away / generalize as ElementwiseBinaryOp<COp>
	}

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices
	 *
	 *	\f$r\_dest = r\_dest + this\f$
	 *
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *	@note This function throws std::bad_alloc.
	 */
	template <class _CBlockMatrixTypelist>
	inline bool AddTo_FBS(CUberBlockMatrix &r_dest) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices with right-side scalar multiplication
	 *
	 *	\f$r\_dest = r\_dest + this \cdot f\_factor\f$
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] f_factor is right-side scalar factor
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This function is slightly slower than AddTo(CUberBlockMatrix&), use that
	 *		where the multiplication is not required.
	 *	@note This leaves r_dest damaged in case the block layout is not compatible
	 *		(the changes are detected inside the loop for performance purposes).
	 */
	inline bool AddTo(CUberBlockMatrix &r_dest, double f_factor) const // throw(std::bad_alloc)
	{
		return ElementwiseBinaryOp_RightSideZeroInvariant(r_dest, CAddWeighted(f_factor));
	}

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices with right-side scalar multiplication
	 *
	 *	\f$r\_dest = r\_dest + this \cdot f\_factor\f$
	 *
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] f_factor is right-side scalar factor
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This function is slightly slower than AddTo(CUberBlockMatrix&), use that
	 *		where the multiplication is not required.
	 *	@note This leaves r_dest damaged in case the block layout is not compatible
	 *		(the changes are detected inside the loop for performance purposes).
	 */
	template <class CBlockMatrixTypelist>
	inline bool AddTo_FBS(CUberBlockMatrix &r_dest, double f_factor) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices with scalar multiplication
	 *
	 *	\f$r\_dest = r\_dest \cdot f\_factor\_dest + this \cdot f\_factor\_this\f$
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] f_factor_dest is left-side scalar factor
	 *	@param[in] f_factor_this is right-side scalar factor
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This function is slightly slower than AddTo(CUberBlockMatrix&, double), use that
	 *		where the left-side multiplication is not required.
	 *	@note This leaves r_dest damaged in case the block layout is not compatible
	 *		(the changes are detected inside the loop for performance purposes).
	 */
	inline bool AddTo(CUberBlockMatrix &r_dest, double f_factor_dest, double f_factor_this) const // throw(std::bad_alloc)
	{
		return ElementwiseBinaryOp(r_dest, CWeightedAddWeighted(f_factor_dest, f_factor_this));
	}

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise addition of two matrices with scalar multiplication
	 *
	 *	\f$r\_dest = r\_dest \cdot f\_factor\_dest + this \cdot f\_factor\_this\f$
	 *
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] f_factor_dest is left-side scalar factor
	 *	@param[in] f_factor_this is right-side scalar factor
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This function is slightly slower than AddTo(CUberBlockMatrix&, double), use that
	 *		where the left-side multiplication is not required.
	 *	@note This leaves r_dest damaged in case the block layout is not compatible
	 *		(the changes are detected inside the loop for performance purposes).
	 */
	template <class CBlockMatrixTypelist>
	inline bool AddTo_FBS(CUberBlockMatrix &r_dest, double f_factor_dest, double f_factor_this) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This version requires that dest == op(dest, 0) and src == op(0, src) holds
	 *		for every possible values of dest and src. Thanks to this limitation, this function
	 *		is slightly faster than ElementwiseBinaryOp_RightSideZeroInvariant().
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBinaryOp>
	bool ElementwiseBinaryOp_ZeroInvariant(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This version requires that dest == op(dest, 0) and src == op(0, src) holds
	 *		for every possible values of dest and src. Thanks to this limitation, this function
	 *		is slightly faster than ElementwiseBinaryOp_RightSideZeroInvariant().
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBlockMatrixTypelist, class CBinaryOp>
	bool ElementwiseBinaryOp_ZeroInvariant_FBS(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This version requires that dest == op(dest, 0) holds
	 *		for every possible value of dest. Thanks to this limitation, this function
	 *		is slightly faster than ElementwiseBinaryOp().
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBinaryOp>
	bool ElementwiseBinaryOp_RightSideZeroInvariant(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This version requires that dest == op(dest, 0) holds
	 *		for every possible value of dest. Thanks to this limitation, this function
	 *		is slightly faster than ElementwiseBinaryOp().
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBlockMatrixTypelist, class CBinaryOp>
	bool ElementwiseBinaryOp_RightSideZeroInvariant_FBS(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBinaryOp>
	bool ElementwiseBinaryOp(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs element-wise binary operation on two matrices
	 *
	 *	\f$r\_dest = op(r\_dest, this)\f$
	 *
	 *	@tparam CBinaryOp is binary function or a binary function object, operating on double.
	 *	@tparam CBlockMatrixTypelist is a typelist, containing Eigen matrices,
	 *		representing the possible block sizes
	 *
	 *	@param[out] r_dest is destination matrix (must have the same dimensions and block layout)
	 *	@param[in] op is binary function pointer or binary functor instance
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note Functor op is always called as dest = op(dest, src) (parameter order is maintained).
	 */
	template <class CBlockMatrixTypelist, class CBinaryOp>
	bool ElementwiseBinaryOp_FBS(CUberBlockMatrix &r_dest, CBinaryOp op) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$this = r\_A \cdot r\_B\f$
	 *
	 *	@param[in] r_A is the left-side matrix
	 *	@param[in] r_B is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls one of \ref MultiplyToWith_LogLookup(), \ref MultiplyToWith_TransposeSort()
	 *		or \ref MultiplyToWith_AccumLookup() based on the configuration (via preprocessor macros).
	 */
	inline bool ProductOf(const CUberBlockMatrix &r_A, const CUberBlockMatrix &r_B); // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$this = r\_A \cdot r\_B\f$
	 *
	 *	@param[in] r_A is the left-side matrix
	 *	@param[in] r_B is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated; otherwise MultiplyToWith() without the third
	 *		argument is faster and gives identical result)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls one of \ref MultiplyToWith_LogLookup(), \ref MultiplyToWith_TransposeSort()
	 *		or \ref MultiplyToWith_AccumLookup() based on the configuration (via preprocessor macros).
	 */
	inline bool ProductOf(const CUberBlockMatrix &r_A, const CUberBlockMatrix &r_B,
		bool b_upper_diag_only); // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs matrix multiplication, this version uses fixed block size
	 *
	 *	\f$this = r\_A \cdot r\_B\f$
	 *
	 *	@tparam CBlockMatrixTypelistA is list of Eigen::Matrix block sizes found in r_A
	 *	@tparam CBlockMatrixTypelistB is list of Eigen::Matrix block sizes found in r_B
	 *
	 *	@param[in] r_A is the left-side matrix
	 *	@param[in] r_B is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistA, class CBlockMatrixTypelistB>
	inline bool ProductOf_FBS(const CUberBlockMatrix &r_A, const CUberBlockMatrix &r_B); // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication, this version uses fixed block size
	 *
	 *	\f$this = r\_A \cdot r\_B\f$
	 *
	 *	@tparam CBlockMatrixTypelistA is list of Eigen::Matrix block sizes found in r_A
	 *	@tparam CBlockMatrixTypelistB is list of Eigen::Matrix block sizes found in r_B
	 *
	 *	@param[in] r_A is the left-side matrix
	 *	@param[in] r_B is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated; otherwise MultiplyToWith() without the third
	 *		argument is faster and gives identical result)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistA, class CBlockMatrixTypelistB>
	inline bool ProductOf_FBS(const CUberBlockMatrix &r_A, const CUberBlockMatrix &r_B, bool b_upper_diag_only); // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs multiplication of matrix transpose and the matrix
	 *
	 *	\f$r\_dest = this^T \cdot this\f$
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *	@param[in] b_upper_diagonal_only is upper diagonal flag (the lower diagonal
	 *		is being explicitly mirrored, and may not be required in some cases
	 *		(e.g. cs_cholesky and CHOLMOD only work with upper/diagonal part)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note Even though b_upper_diagonal_only is set, the result
	 *		still checks out with b_SymmetricLayout().
	 *	@note Note to self: this is almost 2x faster than calling MultiplyWithTo(),
	 *		in case there is no optimization for MultiplyWithTo(), there's little
	 *		reason to optimize this.
	 */
	void PreMultiplyWithSelfTransposeTo(CUberBlockMatrix &r_dest,
		bool b_upper_diagonal_only = false) const; // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs multiplication of matrix transpose and the matrix,
	 *		this version uses fixed block size
	 *
	 *	\f$r\_dest = this^T \cdot this\f$
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *	@param[in] b_upper_diagonal_only is upper diagonal flag (the lower diagonal
	 *		is being explicitly mirrored, and may not be required in some cases
	 *		(e.g. cs_cholesky and CHOLMOD only work with upper/diagonal part)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note Even though b_upper_diagonal_only is set, the result
	 *		still checks out with b_SymmetricLayout().
	 *	@note Note to self: this is almost 2x faster than calling MultiplyWithTo(),
	 *		in case there is no optimization for MultiplyWithTo(), there's little
	 *		reason to optimize this.
	 */
	template <class CBlockMatrixTypelist>
	void PreMultiplyWithSelfTransposeTo_FBS(CUberBlockMatrix &r_dest,
		bool b_upper_diagonal_only = false); // throw(std::bad_alloc)

	/**
	 *	@brief performs multiplication of matrix transpose and the matrix,
	 *		this version uses fixed block size
	 *
	 *	\f$r\_dest = this^T \cdot this\f$
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[out] r_dest is destination matrix (will be overwritten)
	 *	@param[in] b_upper_diagonal_only is upper diagonal flag (the lower diagonal
	 *		is being explicitly mirrored, and may not be required in some cases
	 *		(e.g. cs_cholesky and CHOLMOD only work with upper/diagonal part)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note Even though b_upper_diagonal_only is set, the result
	 *		still checks out with b_SymmetricLayout().
	 *	@note Note to self: this is almost 2x faster than calling MultiplyWithTo(),
	 *		in case there is no optimization for MultiplyWithTo(), there's little
	 *		reason to optimize this.
	 *	@note This version is paralelized using OpenMP (runs in serial where not available).
	 */
	template <class CBlockMatrixTypelist>
	void PreMultiplyWithSelfTransposeTo_FBS_Parallel(CUberBlockMatrix &r_dest,
		bool b_upper_diagonal_only = false); // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls one of \ref MultiplyToWith_LogLookup(), \ref MultiplyToWith_TransposeSort()
	 *		or \ref MultiplyToWith_AccumLookup() based on the configuration (via preprocessor macros).
	 */
	inline bool MultiplyToWith(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls one of \ref MultiplyToWith_LogLookup(), \ref MultiplyToWith_TransposeSort()
	 *		or \ref MultiplyToWith_AccumLookup() based on the configuration (via preprocessor macros).
	 */
	inline bool MultiplyToWith(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using logarithmic time lookup for blocks
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_LogLookup(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using logarithmic time lookup for blocks
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_LogLookup(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using block transpose as a fast sort
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_TransposeSort(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using block transpose as a fast sort
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_TransposeSort(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using a helper dense lookup array
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_AccumLookup(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using a helper dense lookup array
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool MultiplyToWith_AccumLookup(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls either \ref MultiplyToWith_TransposeSort_FBS()
	 *		or \ref MultiplyToWith_AccumLookup_FBS() unless
	 *		<tt>__UBER_BLOCK_MATRIX_LEGACY_FBS_GEMM</tt> is defined.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	inline bool MultiplyToWith_FBS(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated; otherwise MultiplyToWith() without the third
	 *		argument is faster and gives identical result)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This calls either \ref MultiplyToWith_TransposeSort_FBS()
	 *		or \ref MultiplyToWith_AccumLookup_FBS() unless
	 *		<tt>__UBER_BLOCK_MATRIX_LEGACY_FBS_GEMM</tt> is defined.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	inline bool MultiplyToWith_FBS(CUberBlockMatrix &r_dest,
		const CUberBlockMatrix &r_other, bool b_upper_diag_only) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using block transpose as a fast sort
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	inline bool MultiplyToWith_TransposeSort_FBS(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using block transpose as a fast sort
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated; otherwise MultiplyToWith() without the third
	 *		argument is faster and gives identical result)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	bool MultiplyToWith_TransposeSort_FBS(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using a helper dense lookup array
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	inline bool MultiplyToWith_AccumLookup_FBS(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other) const; // throw(std::bad_alloc)

	/**
	 *	@brief performs matrix multiplication using a helper dense lookup array
	 *
	 *	\f$r\_dest = this \cdot r\_other\f$
	 *
	 *	@param[in] r_dest is the destination matrix (will be overwritten)
	 *	@param[in] r_other is the right-side matrix
	 *	@param[in] b_upper_diag_only is upper-triangular flag (if set, only the upper-triangular
	 *		part of the product is calculated; otherwise MultiplyToWith() without the third
	 *		argument is faster and gives identical result)
	 *
	 *	@return Returns true on success, false on failure (incompatible layout).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CBlockMatrixTypelistThis, class CBlockMatrixTypelistOther>
	bool MultiplyToWith_AccumLookup_FBS(CUberBlockMatrix &r_dest, const CUberBlockMatrix &r_other,
		bool b_upper_diag_only) const; // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief calculates inverse of a sparse symmetric block matrix
	 *
	 *	The function looks for independent groups of blocks in the matrix, and
	 *	calculates dense inverse around each block. In case the matrix is fully
	 *	connected, the inverse will be completely dense.
	 *	The use of this function is therefore limited to a few very specific
	 *	cases (Schur complement is one of them). To solve a general system of
	 *	equations, use CholeskyOf().
	 *
	 *	To verify correctness (in debug), use the following code:
	 *	@code
	 *	CUberBlockMatrix A;
	 *	// matrix to be inverted (must have symmetric layout)
	 *
	 *	CUberBlockMatrix A_inv;
	 *	A_inv.InverseOf_Symmteric(A);
	 *	// calculates the inverse of A
	 *
	 *	CUberBlockMatrix prod;
	 *	prod.ProductOf(A, A_inv);
	 *	// multiply prod = A * A^{-1}
	 *
	 *	CUberBlockMatrix identity;
	 *	A.CopyLayoutTo(identity);
	 *	identity.SetIdentity();
	 *	// create identity matrix of the same layout as A
	 *
	 *	prod.AddTo(identity, -1);
	 *	// subtract product from identity
	 *
	 *	double f_error = identity.f_Norm();
	 *	// calculate error as ||I - A * A^{-1}||.
	 *	@endcode
	 *
	 *	@param[in] r_A is the input matrix (must be invertible and full rank)
	 *	@param[in] b_upper_triangular_source is upper triangular source flag
	 *		(set if only the upper triangular part of r_A is stored)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note In case the matrix is not invertible, the result
	 *		is undefined (no exception is thrown).
	 *	@note Actually, only the layout needs to be symmetric,
	 *		the nonzero blocks can be scattered at will,
	 *		the content does not need to be symmetric at all.
	 *
	 *	@todo Test it on some data, it is largerly untested.
	 */
	// *	t_odo Implement InverseOf_SymmtericBlockDiagonal(), implement FBS versions of both.
	// *	t_odo Often, symmetric matrices are only represented by the upper triangular,
	// *		need to add parameter to mirror the blocks below the diagonal as well.
	void InverseOf_Symmteric(const CUberBlockMatrix &r_A, bool b_upper_triangular_source = false); // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief calculates inverse of a sparse symmetric block matrix
	 *
	 *	The function looks for independent groups of blocks in the matrix, and
	 *	calculates dense inverse around each block. In case the matrix is fully
	 *	connected, the inverse will be completely dense.
	 *	The use of this function is therefore limited to a few very specific
	 *	cases (Schur complement is one of them). To solve a general system of
	 *	equations, use CholeskyOf().
	 *
	 *	To verify correctness (in debug), use the following code:
	 *	@code
	 *	CUberBlockMatrix A;
	 *	// matrix to be inverted (must have symmetric layout)
	 *
	 *	CUberBlockMatrix A_inv;
	 *	A_inv.InverseOf_Symmteric(A);
	 *	// calculates the inverse of A
	 *
	 *	CUberBlockMatrix prod;
	 *	prod.ProductOf(A, A_inv);
	 *	// multiply prod = A * A^{-1}
	 *
	 *	CUberBlockMatrix identity;
	 *	A.CopyLayoutTo(identity);
	 *	identity.SetIdentity();
	 *	// create identity matrix of the same layout as A
	 *
	 *	prod.AddTo(identity, -1);
	 *	// subtract product from identity
	 *
	 *	double f_error = identity.f_Norm();
	 *	// calculate error as ||I - A * A^{-1}||.
	 *	@endcode
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[in] r_A is the input matrix (must be invertible and full rank)
	 *	@param[in] b_upper_triangular_source is upper triangular source flag
	 *		(set if only the upper triangular part of r_A is stored)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note In case the matrix is not invertible, the result
	 *		is undefined (no exception is thrown).
	 *	@note Actually, only the layout needs to be symmetric,
	 *		the nonzero blocks can be scattered at will,
	 *		the content does not need to be symmetric at all.
	 *
	 *	@todo Test it on some data, it is largerly untested.
	 */
	// *	t_odo Often, symmetric matrices are only represented by the upper triangular,
	// *		need to add parameter to mirror the blocks below the diagonal as well.
	template <class CBlockMatrixTypelist>
	void InverseOf_Symmteric_FBS(const CUberBlockMatrix &r_A, bool b_upper_triangular_source = false); // throw(std::bad_alloc)

	/**
	 *	@brief calculates inverse of a sparse block diagonal matrix
	 *
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 *
	 *	@param[in] r_A is the input matrix (must be invertible, can point to this)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note In case the matrix is not invertible, the result
	 *		is undefined (no exception is thrown).
	 *	@note Actually, only the layout needs to be symmetric,
	 *		the nonzero blocks can be scattered at will,
	 *		the content does not need to be symmetric at all.
	 */
	template <class CBlockMatrixTypelist>
	void InverseOf_BlockDiag_FBS_Parallel(const CUberBlockMatrix &r_A); // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief permutes right-hand side vector by blockwise permutation
	 *
	 *	This avoids creating elementwise permutation to permute a dense vector,
	 *	matrix layout is used instead. This version uses row layout.
	 *
	 *	@param[out] p_dest is destination vector (must be allocated
	 *		to n_vector_length elements, will be overwritten)
	 *	@param[in] p_src is source vector (must be allocated to n_vector_length elements)
	 *	@param[in] n_vector_length is number of elements of the dense vector
	 *		(must be equal to or less than the number of matrix columns,
	 *		depending on n_permutation_length)
	 *	@param[in] p_permutation is blockwise permutation vector
	 *	@param[in] n_permutation_length is number of blockwise permutation elements
	 *		(must be equal to or less than the number of matrix block columns)
	 *
	 *	@note With this function, it is possible to permute shorter vectors
	 *		than the size of the matrix, the result is the same as the corresponding
	 *		head of the vector of the same length as the matrix. It is important
	 *		that the vector lenght exactly matches the sum of sizes of blocks
	 *		selected by the permutation vector.
	 */
	void Permute_RightHandSide_Vector(double *p_dest, const double *p_src,
		size_t UNUSED(n_vector_length), const size_t *p_permutation,
		size_t n_permutation_length) const;

	/**
	 *	@brief permutes left-hand side vector by blockwise permutation
	 *
	 *	This avoids creating elementwise permutation to permute a dense vector,
	 *	matrix layout is used instead. This version uses column layout.
	 *
	 *	@param[out] p_dest is destination vector (must be allocated
	 *		to n_vector_length elements, will be overwritten)
	 *	@param[in] p_src is source vector (must be allocated to n_vector_length elements)
	 *	@param[in] n_vector_length is number of elements of the dense vector
	 *		(must be equal to or less than the number of matrix columns,
	 *		depending on n_permutation_length)
	 *	@param[in] p_permutation is blockwise permutation vector
	 *	@param[in] n_permutation_length is number of blockwise permutation elements
	 *		(must be equal to or less than the number of matrix block columns)
	 *
	 *	@note With this function, it is possible to permute shorter vectors
	 *		than the size of the matrix, the result is the same as the corresponding
	 *		head of the vector of the same length as the matrix. It is important
	 *		that the vector lenght exactly matches the sum of sizes of blocks
	 *		selected by the permutation vector.
	 */
	void Permute_LeftHandSide_Vector(double *p_dest, const double *p_src,
		size_t UNUSED(n_vector_length), const size_t *p_permutation,
		size_t n_permutation_length) const;

	/**
	 *	@brief inversely permutes right-hand side vector by blockwise permutation
	 *
	 *	This avoids creating elementwise permutation to permute a dense vector,
	 *	matrix layout is used instead. This version uses row layout.
	 *
	 *	@param[out] p_dest is destination vector (must be allocated
	 *		to n_vector_length elements, will be overwritten)
	 *	@param[in] p_src is source vector (must be allocated to n_vector_length elements)
	 *	@param[in] n_vector_length is number of elements of the dense vector
	 *		(must equal to the number of matrix rows)
	 *	@param[in] p_permutation is blockwise permutation vector
	 *	@param[in] n_permutation_length is number of blockwise permutation elements
	 *		(must equal to the number of matrix block rows)
	 */
	void InversePermute_RightHandSide_Vector(double *p_dest, const double *p_src,
		size_t UNUSED(n_vector_length), const size_t *p_permutation,
		size_t n_permutation_length) const;

	/**
	 *	@brief inversely permutes left-hand side vector by blockwise permutation
	 *
	 *	This avoids creating elementwise permutation to permute a dense vector,
	 *	matrix layout is used instead. This version uses column layout.
	 *
	 *	@param[out] p_dest is destination vector (must be allocated
	 *		to n_vector_length elements, will be overwritten)
	 *	@param[in] p_src is source vector (must be allocated to n_vector_length elements)
	 *	@param[in] n_vector_length is number of elements of the dense vector
	 *		(must equal to the number of matrix columns)
	 *	@param[in] p_permutation is blockwise permutation vector
	 *	@param[in] n_permutation_length is number of blockwise permutation elements
	 *		(must equal to the number of matrix block columns)
	 */
	void InversePermute_LeftHandSide_Vector(double *p_dest, const double *p_src,
		size_t UNUSED(n_vector_length), const size_t *p_permutation,
		size_t n_permutation_length) const;

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	bool UpperTriangularTranspose_Solve_FBS(double *p_x, size_t UNUSED(n_vector_size)) const;

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	This version allows for "resumed forward" substitution. In that case, n_skip_columns is greater
	 *	than zero, and p_x contains the solution from the last step in elements 0 to
	 *	n_BlockColumn_Base(n_skip_columns) and a corresponding part of the right-hand side vector in the
	 *	rest of the elements. Doing that avoids recalculating the prefix of the solution in cases where
	 *	there is a common prefix in both right-hand side vector and the matrix, because then only the
	 *	suffix of the sollution changes.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_skip_columns is number of matrix columns to skip (in blocks)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	bool UpperTriangularTranspose_Solve_FBS(double *p_x, size_t UNUSED(n_vector_size), size_t n_skip_columns) const;

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] p_dependent_column is pointer to the array of dependent column
	 *		indices (must be in strictly ascending order)
	 *	@param[in] n_dependent_column_num is number of the block columns that the result
	 *		depends on (one to the size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	bool UpperTriangularTranspose_Solve_FBS(double *p_x, size_t UNUSED(n_vector_size),
		const size_t *p_dependent_column, size_t n_dependent_column_num) const;

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	inline bool UpperTriangular_Solve_FBS(double *p_x, size_t n_vector_size) const
	{
		return UpperTriangular_Solve_FBS<CBlockMatrixTypelist>(p_x,
			n_vector_size, 0, m_block_cols_list.size() - 1);
	}

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_first_column is zero-based index of the first block column to start
	 *		backsubstitution at (the corresponding tail of p_x is supposed to be zero)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	inline bool UpperTriangular_Solve_FBS(double *p_x, size_t n_vector_size, size_t n_first_column) const
	{
		return UpperTriangular_Solve_FBS<CBlockMatrixTypelist>(p_x,
			n_vector_size, 0, n_first_column);
	}

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@tparam CBlockMatrixTypelist is typelist, containing Eigen
	 *		matrices with known compile-time sizes
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_last_column is zero-based index of the last block column to perform
	 *		backsubstitution (zero by default, it is inclusive: the last is also processed)
	 *	@param[in] n_first_column is zero-based index of the first block column to start
	 *		backsubstitution at (the corresponding tail of p_x is supposed to be zero)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	template <class CBlockMatrixTypelist>
	bool UpperTriangular_Solve_FBS(double *p_x, size_t UNUSED(n_vector_size),
		size_t n_last_column, size_t n_first_column) const;

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	bool UpperTriangularTranspose_Solve(double *p_x, size_t UNUSED(n_vector_size)) const;

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] p_dependent_column is pointer to the array of dependent column
	 *		indices (must be in strictly ascending order)
	 *	@param[in] n_dependent_column_num is number of the block columns that the result
	 *		depends on (one to the size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	bool UpperTriangularTranspose_Solve(double *p_x, size_t UNUSED(n_vector_size),
		const size_t *p_dependent_column, size_t n_dependent_column_num) const; // todo - add sparse sparse backsubsitution versions as well, think about vectorized versions (multiple right hand sides as an Eigen::Matrix)

	/**
	 *	@brief solves a system given by transpose of upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this^T \cdot x = b\f$ (this matrix is just upper triangular, the transpose is calculated
	 *	inside the function), where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact forward substitution.
	 *
	 *	This version allows for "resumed forward" substitution. In that case, n_skip_columns is greater
	 *	than zero, and p_x contains the solution from the last step in elements 0 to
	 *	n_BlockColumn_Base(n_skip_columns) and a corresponding part of the right-hand side vector in the
	 *	rest of the elements. Doing that avoids recalculating the prefix of the solution in cases where
	 *	there is a common prefix in both right-hand side vector and the matrix, because then only the
	 *	suffix of the sollution changes.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_skip_columns is number of matrix columns to skip (in blocks)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *	@note The transpose is not really being calculated, it is carried out using the indexing.
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	bool UpperTriangularTranspose_Solve(double *p_x, size_t UNUSED(n_vector_size), size_t n_skip_columns) const;

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	inline bool UpperTriangular_Solve(double *p_x, size_t UNUSED(n_vector_size)) const
	{
		return UpperTriangular_Solve(p_x, n_vector_size, 0, m_block_cols_list.size() - 1);
		// no performance penalty involved here
	}

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_first_column is zero-based index of the first block column to start
	 *		backsubstitution at (the corresponding tail of p_x is supposed to be zero)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	inline bool UpperTriangular_Solve(double *p_x, size_t UNUSED(n_vector_size), size_t n_first_column) const
	{
		return UpperTriangular_Solve(p_x, n_vector_size, 0, n_first_column);
		// no performance penalty involved here
	}

	/**
	 *	@brief solves a system given by upper triangular matrix and a right-hand side vector
	 *
	 *	Solves \f$this \cdot x = b\f$, where p_x is \f$b\f$ on input, \f$x\f$ on output. This is in fact back substitution.
	 *
	 *	@param[in,out] p_x is the right-hand side vector (gets overwritten by the solution)
	 *	@param[in] n_vector_size is the size of the right-hand side vector, in elements
	 *		(must match size of this matrix)
	 *	@param[in] n_last_column is zero-based index of the last block column to perform
	 *		backsubstitution (zero by default, it is inclusive: the last is also processed)
	 *	@param[in] n_first_column is zero-based index of the first block column to start
	 *		backsubstitution at (the corresponding tail of p_x is supposed to be zero)
	 *
	 *	@return Returns true on success, false on failure (no solution or infinite number of solutions).
	 *
	 *	@note This function requires the matrix to be square, have symmetric layout
	 *		and to be upper triangular (all the blocks on the diagonal must be square).
	 *
	 *	@todo Add support for block permutation (2nd function), should be faster than permuting
	 *		p_x there and back again (measure, measure).
	 */
	bool UpperTriangular_Solve(double *p_x, size_t UNUSED(n_vector_size),
		size_t n_last_column, size_t n_first_column) const;

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief calculates inplace (upper) Cholesky factorization using Eigen dense matrix
	 *
	 *	@tparam CMatrixBlockSizeList is a list of possible matrix block sizes
	 *	@tparam n_max_matrix_size is maximal size of matrix to allow in the static
	 *		decission tree (directly affects speed of compilation)
	 *
	 *	@return Returns true on success, false on failure (not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note To calculate sparse cholesky, use e.g. CholeskyOf().
	 *	@note This function throws std::bad_alloc.
	 */
	template <class CMatrixBlockSizeList, const int n_max_matrix_size>
	bool Cholesky_Dense_FBS(); // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@brief calculates inplace (upper) Cholesky factorization using Eigen dense matrix
	 *
	 *	@tparam MatrixRowsAtCompileTime is number of matrix rows
	 *		(if known at compile time - otherwise Eigen::Dynamic)
	 *	@tparam MatrixColumnsAtCompileTime is number of matrix columns
	 *		(if known at compile time - otherwise Eigen::Dynamic)
	 *
	 *	@return Returns true on success, false on failure (not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note To calculate sparse cholesky, use e.g. CholeskyOf().
	 */
	template <const int MatrixRowsAtCompileTime>
	bool Cholesky_Dense(); // throw(std::bad_alloc)

	/**
	 *	@brief builds an elimination tree for Cholesky factorization
	 *
	 *	This matrix needs to be symmetric in order to calculate the elimination tree.
	 *
	 *	@param[out] r_elim_tree is vector with elimination tree (contains block column elements)
	 *	@param[out] r_workspace is workspace vector (will be allocated to block column elements)
	 *
	 *	@note Only the upper-triangular part of this matrix is used (and only the block
	 *		structure is used, the values are irrelevant).
	 *	@note This function throws std::bad_alloc.
	 */
	void Build_EliminationTree(std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_workspace) const; // throw(std::bad_alloc)

	/**
	 *	@brief calculates column dependencies from elimination tree
	 *
	 *	@param[in] n_column_id is zero-based index of a column to calculate dependencies for
	 *	@param[in] r_elim_tree is an elimination tree for matrix with the same layout as this
	 *		(e.g. obtained by a call to Build_EliminationTree())
	 *	@param[in,out] r_ereach_stack is vector that will contain the dependences upon return,
	 *		must be allocated to block column elements
	 *	@param[in,out] r_workspace is workspace vector, must be allocated to block column
	 *		elements and all the elements must be null (will be null again upon return)
	 *
	 *	@return Returns (zero-based) index into r_ereach_stack where the first column
	 *		dependence is stored (the list ends at the number of block columns).
	 */
	size_t n_Build_EReach(size_t n_column_id, const std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_ereach_stack, std::vector<size_t> &r_workspace) const;

	/**
	 *	@brief calculates (upper) Cholesky factorization of a given sparse block matrix
	 *
	 *	@param[in] r_lambda is the input matrix (must be positive definite)
	 *
	 *	@return Returns true on success, false on failure (lambda is not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note If factorizations of multiple matrices with the same layout are required,
	 *		use the CholeskyOf(const CUberBlockMatrix&, const std::vector<size_t>&,
	 *		std::vector<size_t>&, std::vector<size_t>&) variant of this function.
	 */
	inline bool CholeskyOf(const CUberBlockMatrix &r_lambda); // throw(std::bad_alloc)

	/**
	 *	@brief calculates resumed (upper) Cholesky factorization of a given sparse block matrix
	 *
	 *	This enables partial recalculation of Cholesky factor, given that the original matrix
	 *	has unchanged prefix of n_start_on_column rows and columns. This also enables updating
	 *	Cholesky factor after the original matrix grew in size.
	 *
	 *	If the followitg two conditions are met:
	 *		* this must be Cholesky factorization of some original lambda.
	 *		* r_lambda must have n_start_on_column block rows and block columns identical to the original lambda.
	 *
	 *	Then upon successful completion of this function, this is Cholesky factorization of r_lambda,
	 *	calculated in time proportional to r_lambda.n_BlockColumn_Num() - n_start_on_column
	 *	(ie. smaller time than full Cholesky, given n_start_on_column is greater than zero).
	 *
	 *	@param[in] r_lambda is the input matrix (must be positive definite)
	 *	@param[in] n_start_on_column is zero-based index of the first block column
	 *		of the factor to be recalculated
	 *
	 *	@return Returns true on success, false on failure (lambda is not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note If factorizations of multiple matrices with the same layout are required,
	 *		use the CholeskyOf(const CUberBlockMatrix&, const std::vector<size_t>&,
	 *		std::vector<size_t>&, std::vector<size_t>&, size_t) variant of this function.
	 */
	inline bool CholeskyOf(const CUberBlockMatrix &r_lambda, size_t n_start_on_column); // throw(std::bad_alloc)

	/**
	 *	@brief calculates (upper) Cholesky factorization of a given sparse block matrix
	 *
	 *	@param[in] r_lambda is the input matrix (must be positive definite)
	 *	@param[in] r_elim_tree is an elimination tree of r_lambda
	 *		(or of a matrix with the same block structure)
	 *	@param[in,out] r_workspace is a vector allocated to number of block columns of r_lambda
	 *	@param[in,out] r_zero_workspace is a vector allocated to number of block columns
	 *		of r_lambda and cleared to all zeros
	 *
	 *	@return Returns true on success, false on failure (lambda is not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool CholeskyOf(const CUberBlockMatrix &r_lambda, const std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_workspace, std::vector<size_t> &r_zero_workspace); // throw(std::bad_alloc)

	/**
	 *	@brief calculates resumed (upper) Cholesky factorization of a given sparse block matrix
	 *
	 *	This enables partial recalculation of Cholesky factor, given that the original matrix
	 *	has unchanged prefix of n_start_on_column rows and columns. This also enables updating
	 *	Cholesky factor after the original matrix grew in size.
	 *
	 *	If the followitg two conditions are met:
	 *		* this must be Cholesky factorization of some original lambda.
	 *		* r_lambda must have n_start_on_column block rows and block columns identical to the original lambda.
	 *
	 *	Then upon successful completion of this function, this is Cholesky factorization of r_lambda,
	 *	calculated in time proportional to r_lambda.n_BlockColumn_Num() - n_start_on_column
	 *	(ie. smaller time than full Cholesky, given n_start_on_column is greater than zero).
	 *
	 *	@param[in] r_lambda is the input matrix (must be positive definite)
	 *	@param[in] r_elim_tree is an elimination tree of r_lambda
	 *		(or of a matrix with the same block structure)
	 *	@param[in,out] r_workspace is a vector allocated to number of block columns of r_lambda
	 *	@param[in,out] r_zero_workspace is a vector allocated to number of block columns
	 *		of r_lambda and cleared to all zeros
	 *	@param[in] n_start_on_column is zero-based index of the first block column
	 *		of the factor to be recalculated
	 *
	 *	@return Returns true on success, false on failure (lambda is not positive definite).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool CholeskyOf(const CUberBlockMatrix &r_lambda, const std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_workspace, std::vector<size_t> &r_zero_workspace, size_t n_start_on_column); // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	/**
	 *	@copydoc CholeskyOf(const CUberBlockMatrix&)
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 */
	template <class CBlockMatrixTypelist>
	bool CholeskyOf_FBS(const CUberBlockMatrix &r_lambda); // throw(std::bad_alloc)

	/**
	 *	@copydoc CholeskyOf(const CUberBlockMatrix&, size_t)
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 */
	template <class CBlockMatrixTypelist>
	bool CholeskyOf_FBS(const CUberBlockMatrix &r_lambda, size_t n_start_on_column); // throw(std::bad_alloc)

	/**
	 *	@copydoc CholeskyOf(const CUberBlockMatrix&, const std::vector<size_t>&, std::vector<size_t>&, std::vector<size_t>&)
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 */
	template <class CBlockMatrixTypelist>
	bool CholeskyOf_FBS(const CUberBlockMatrix &r_lambda, const std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_workspace, std::vector<size_t> &r_zero_workspace); // throw(std::bad_alloc)

	/**
	 *	@copydoc CholeskyOf(const CUberBlockMatrix&, const std::vector<size_t>&, std::vector<size_t>&, std::vector<size_t>&, size_t)
	 *	@tparam CBlockMatrixTypelist is list of Eigen::Matrix block sizes
	 */
	template <class CBlockMatrixTypelist>
	bool CholeskyOf_FBS(const CUberBlockMatrix &r_lambda, const std::vector<size_t> &r_elim_tree,
		std::vector<size_t> &r_workspace, std::vector<size_t> &r_zero_workspace,
		size_t n_start_on_column); // throw(std::bad_alloc)

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	bool LUTo(CUberBlockMatrix &r_L, CUberBlockMatrix &r_U, size_t *p_row_perm, size_t *p_col_perm,
		bool *p_pivoted = 0, bool b_partial_interblock_pivoting = true, double f_min_piv_gain = 0,
		bool b_full_intrablock_pivoting = true) const;

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

	template <class CBlockMatrixTypelist>
	bool LUTo_FBS(CUberBlockMatrix &r_L, CUberBlockMatrix &r_U, size_t *p_row_perm, size_t *p_col_perm,
		bool *p_pivoted = 0, bool b_partial_interblock_pivoting = true, double f_min_piv_gain = 0,
		bool b_full_intrablock_pivoting = true) const;

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

#ifdef __UBER_BLOCK_MATRIX_IO

	/**
	 *	@brief loads block layout from a file
	 *
	 *	@param[in] p_s_layout_filename is output file name for the block layout (.bla)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note In case it fails, the matrix is not modified,
	 *		otherwise the matrix is empty and has the specified layout.
	 *	@note This function throws std::bad_alloc.
	 */
	bool Load_BlockLayout(const char *p_s_layout_filename); // throw(std::bad_alloc)

	/**
	 *	@brief loads a matrix from a file
	 *
	 *	@param[in] p_s_filename is input file name in matrix marked format (.mtx)
	 *	@param[in] p_s_layout_filename is input file name for the block layout (.bla)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	inline bool Load_MatrixMarket(const char *p_s_filename,
		const char *p_s_layout_filename); // throw(std::bad_alloc)

	/**
	 *	@brief loads a matrix from a file
	 *
	 *	@param[in] p_s_filename is input file name in matrix marked format (.mtx)
	 *	@param[in] n_block_size is size of the matrix blocks (all blocks the same size)
	 *	@param[in] b_allow_underfilled_last_block is size tolerance flag (if set and the
	 *		matrix size in the file is not a multiple of the given block size, the
	 *		matrix will be zero-padded from the right and bottom; if not set and the
	 *		size is not a multiple, the function fails and the original matrix is not
	 *		modified)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool Load_MatrixMarket(const char *p_s_filename, size_t n_block_size,
		bool b_allow_underfilled_last_block); // throw(std::bad_alloc)

	/**
	 *	@brief loads a matrix from a file, assuming the layout is ready
	 *
	 *	@param[in] p_s_filename is input file name in matrix marked format (.mtx)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	bool Load_MatrixMarket_Layout(const char *p_s_filename); // throw(std::bad_alloc)

	/**
	 *	@brief writes block layout of this matrix to a file
	 *	@param[in] p_s_layout_filename is output file name for the block layout (.bla)
	 *	@return Returns true on success, false on failure.
	 */
	bool Save_BlockLayout(const char *p_s_layout_filename) const;

	/**
	 *	@brief writes this matrix to a file in matrix market format
	 *
	 *	@param[in] p_s_filename is output file name (.mtx)
	 *	@param[in] p_s_layout_filename is output file
	 *		name for the block layout (.bla, can be null)
	 *	@param[in] p_s_kind is comment in the file header, it is usual to specify
	 *		what type of the matrix is in the file (symmetric, positive definite, etc)
	 *	@param[in] p_s_mm_header is MatrixMarket specific header (will affect the
	 *		parsing of the matrix, does not affect how the matrix is stored)
	 *	@param[in] n_symmetry is symmetry option ('U' for symmetric, sourrce elements
	 *		from the upper triangular, 'L' for elements from lower triangular part
	 *		or 'N' for not symmetric matrices; this is case insensitive)
	 *
	 *	@return Returns true on success, false on failure.
	 *
	 *	@note Symmetric matrices should have header set to "symmetric", "skew-symmetric" or "Hermitian".
	 */
	bool Save_MatrixMarket(const char *p_s_filename,
		const char *p_s_layout_filename = 0,
		const char *p_s_kind = "general block matrix",
		const char *p_s_mm_header = "matrix coordinate real general",
		char n_symmetry = 'N') const;

#endif // __UBER_BLOCK_MATRIX_IO

protected:
	/**
	 *	@brief copies column layout structure without copying the blocks
	 *	@param[in] r_t_src is column layout structure
	 *	@return Returns column layout structure with the blocks ommited.
	 */
	static inline TColumn t_ColumnCumsumCopy(const TColumn &r_t_src);

	/**
	 *	@brief converts column layout structure to row
	 *	@param[in] r_t_src is column layout structure
	 *	@return Returns row layout structure (transpose of input).
	 */
	static inline TRow t_ColumnCumsumToRowCumsum(const TColumn &r_t_src);

	/**
	 *	@brief converts row layout structure to column
	 *	@param[in] r_t_src is row layout structure
	 *	@return Returns column layout structure (transpose of input).
	 */
	static inline TColumn t_RowCumsumToColumnCumsum(const TRow &r_t_src);

	/**
	 *	@brief a simple function object used to recalculate block row cumsums after row permutation
	 */
	class CRecalcRowCumsum { // t_odo - document this
	protected:
		size_t m_n_last_cumsum; /**< @brief cumsum accumulator (the sum of heights of all the processed edges) */

	public:
		/**
		 *	@brief default constructor; clears the cumsum accumulator to zero
		 */
		inline CRecalcRowCumsum()
			:m_n_last_cumsum(0)
		{}

		/**
		 *	@brief function operator; fills row with appropriate cumsum
		 *	@param[in] r_t_row is the row (actually only its height is significant)
		 *	@return Returns row with the given height and with correct height cumsum.
		 */
		inline TRow operator ()(const TRow &r_t_row)
		{
			TRow t_row;
			t_row.n_height = r_t_row.n_height; // copy dimension
			t_row.n_cumulative_height_sum = m_n_last_cumsum; // use the last cumsum
			m_n_last_cumsum += r_t_row.n_height; // update cumsum with block size
			return t_row;
		}

		/**
		 *	@brief conversion to integer
		 *	@return Returns the cumsum of all the processed edges
		 *		(equals height of the matrix, in elements).
		 */
		inline operator size_t() const
		{
			return m_n_last_cumsum;
		}
	};

	/**
	 *	@brief performs addition of two scalars
	 *
	 *	@param[in] f_a is left-side operand
	 *	@param[in] f_b is right-side operand
	 *
	 *	@return Returns the sum of f_a + f_b.
	 */
	static inline double f_Add(double f_a, double f_b)
	{
		return f_a + f_b;
	}

	/**
	 *	@brief determines whether a row is referenced by columns
	 *	@param[in] n_row_index is (zero-based) row index
	 *	@return Returns true if there are some blocks, referencing
	 *		the specified row, otherwise returns false.
	 *	@note This operation is O(n) in number of blocks (slow).
	 */
	bool b_RowReferenced(size_t n_row_index) const;

	/**
	 *	@brief shifts row references after a row was subdivided to two
	 *	@param[in] n_row_index is the index where the new row was inserted;
	 *		indices greater or equal are incremented
	 *	@note This operation is O(n) in number of blocks (slow).
	 */
	void Shift_RowReferences(size_t n_row_index);

	/**
	 *	@brief shifts row references after a row was subdivided to three
	 *
	 *	@param[in] n_row_index0 is the index where the first of the new rows was inserted;
	 *		indices greater or equal are incremented
	 *	@param[in] n_row_index1 is the index where the second of the new rows was inserted;
	 *		currently it must equal n_row_index0 + 2 (limited use case, enables optimizations)
	 *
	 *	@note This operation is O(n) in number of blocks (slow).
	 */
	void Shift_RowReferences2(size_t n_row_index0, size_t UNUSED(n_row_index1));

	/**
	 *	@brief merges matrix layouts in one dimension (either of them, can combine)
	 *
	 *	@tparam _TyDest is destination layout block type (TRow or TColumn)
	 *	@tparam _TyFirst is first layout block type (TRow or TColumn)
	 *	@tparam _TySecond is second layout block type (TRow or TColumn)
	 *	@param[out] r_merged_layout is merged layout (always exists)
	 *	@param[in] r_layout_0 is matrix layout (horizontal or vertical)
	 *	@param[in] r_layout_1 is matrix layout (horizontal or vertical), it must have the same
	 *		number of matrix elements as r_layout_0 (where number of elements is sum of all block sizes)
	 *	@param[out] r_reindexing_table_0 is reindexing table (has the same number of entries as r_layout_0
	 *		and contains indices to merged_layout, or -1 in case the block in question was subdivided)
	 *	@param[out] r_reindexing_table_1 is reindexing table (has the same number of entries as r_layout_1
	 *		and contains indices to merged_layout, or -1 in case the block in question was subdivided)
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class _TyDest, class _TyFirst, class _TySecond>
	static void MergeLayout(std::vector<_TyDest> &r_merged_layout, const std::vector<_TyFirst> &r_layout_0,
		const std::vector<_TySecond> &r_layout_1, std::vector<size_t> &r_reindexing_table_0,
		std::vector<size_t> &r_reindexing_table_1); // throw(std::bad_alloc)

	/**
	 *	@brief merges matrix layouts in one dimension (either of them, can combine)
	 *
	 *	This version just builds reindexing tables, the layout itself is not required.
	 *
	 *	@tparam _TyFirst is first layout block type (TRow or TColumn)
	 *	@tparam _TySecond is second layout block type (TRow or TColumn)
	 *	@param[in] r_layout_0 is matrix layout (horizontal or vertical)
	 *	@param[in] r_layout_1 is matrix layout (horizontal or vertical), it must have the same
	 *		number of matrix elements as r_layout_0 (where number of elements is sum of all block sizes)
	 *	@param[out] r_reindexing_table_0 is reindexing table (has the same number of entries as r_layout_0
	 *		and contains indices to merged_layout, or -1 in case the block in question was subdivided)
	 *	@param[out] r_reindexing_table_1 is reindexing table (has the same number of entries as r_layout_1
	 *		and contains indices to merged_layout, or -1 in case the block in question was subdivided)
	 *
	 *	@return Returns the size of merged layout, would it be built.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	template <class _TyFirst, class _TySecond>
	static size_t n_MergeLayout(const std::vector<_TyFirst> &r_layout_0,
		const std::vector<_TySecond> &r_layout_1, std::vector<size_t> &r_reindexing_table_0,
		std::vector<size_t> &r_reindexing_table_1); // throw(std::bad_alloc)

	/**
	 *	@brief tries to find a row by first element offset
	 *		and block size, creates a new row if needed
	 *
	 *	@param[in] n_row is (zero-based) index of the first block-row element
	 *	@param[in] n_block_row_num is number of elements in the block-row
	 *
	 *	@return Returns (zero-based) block-row index on success, or -1 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	size_t n_RowAlloc(size_t n_row, size_t n_block_row_num); // throw(std::bad_alloc)

	/**
	 *	@brief tries to find a column by first element offset
	 *		and block size, creates a new column if needed
	 *
	 *	@param[in] n_column is (zero-based) index of the first block-column element
	 *	@param[in] n_block_column_num is number of elements in the block-column
	 *
	 *	@return Returns (zero-based) block-column index on success, or -1 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	size_t n_ColumnAlloc(size_t n_column, size_t n_block_column_num); // throw(std::bad_alloc)

	/**
	 *	@brief tries to find a row by first element offset, fails if it doesn't exist
	 *
	 *	@param[in] n_row is (zero-based) index of the first block-row element
	 *	@param[out] r_n_block_row_num is number of elements in the block-row
	 *
	 *	@return Returns (zero-based) block-row index on success, or -1 on failure.
	 */
	inline size_t n_RowGet(size_t n_row, size_t &r_n_block_row_num) const;

	/**
	 *	@brief tries to find a column by first element offset, fails if it doesn't exist
	 *
	 *	@param[in] n_column is (zero-based) index of the first block-column element
	 *	@param[out] r_n_block_column_num is number of elements in the block-column
	 *
	 *	@return Returns (zero-based) block-column index on success, or -1 on failure.
	 */
	inline size_t n_ColumnGet(size_t n_column, size_t &r_n_block_column_num) const;

	/**
	 *	@brief tries to find a block by block position and size, allocates a new one if not found
	 *
	 *	@param[in] n_row_index is (zero-based) index of block-row
	 *	@param[in,out] r_t_col is reference to the block-column
	 *	@param[in] n_block_row_num is number of block rows
	 *	@param[in] n_block_column_num is number of block columns (must match r_t_col.n_width)
	 *	@param[out] r_b_was_a_new_block is set to true in case the block was created,
	 *		otherwise it's set to false (allows for custom block initialization)
	 *
	 *	@return Returns pointer to block data on success, or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	double *p_AllocBlockData(size_t n_row_index, TColumn &r_t_col,
		size_t n_block_row_num, size_t n_block_column_num, bool &r_b_was_a_new_block); // throw(std::bad_alloc)

	/**
	 *	@brief tries to find a block by block position and size, allocates a new one if not found
	 *
	 *	@param[in] n_row_index is (zero-based) index of block-row
	 *	@param[in] n_column_index is (zero-based) index of block-column
	 *	@param[in] n_block_row_num is number of block rows
	 *	@param[in] n_block_column_num is number of block columns
	 *	@param[out] r_b_was_a_new_block is set to true in case the block was created,
	 *		otherwise it's set to false (allows for custom block initialization)
	 *
	 *	@return Returns pointer to block data on success, or 0 on failure.
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	inline double *p_AllocBlockData(size_t n_row_index, size_t n_column_index,
		size_t n_block_row_num, size_t n_block_column_num, bool &r_b_was_a_new_block); // throw(std::bad_alloc)

	/**
	 *	@brief tries to find a block by block position, fails if not found
	 *
	 *	@param[in] n_row_index is (zero-based) index of block-row
	 *	@param[in] n_column_index is (zero-based) index of block-column
	 *
	 *	@return Returns const pointer to block data on success, or 0 in case the block doesn't exist.
	 */
	const double *p_GetBlockData(size_t n_row_index, size_t n_column_index) const;

	/**
	 *	@brief tries to find a block by block position, fails if not found
	 *
	 *	@param[in] n_row_index is (zero-based) index of block-row
	 *	@param[in] n_column_index is (zero-based) index of block-column
	 *
	 *	@return Returns pointer to block data on success, or 0 in case the block doesn't exist.
	 */
	double *p_GetBlockData(size_t n_row_index, size_t n_column_index);

	/**
	 *	@brief builds merged rows and columns layouts for matrix addition
	 *
	 *	@param[in] r_row_list_first is list of block-rows of the first operand
	 *	@param[in] r_column_list_first is list of block-columns of the first operand
	 *	@param[in,out] r_row_list_second is list of block-rows of the second operand
	 *	@param[in,out] r_column_list_second is list of block-columns of the second operand
	 *	@param[out] r_reindex_rows_first is mapping function of row layout of first operand
	 *		to (the merged) second operand row layout
	 *	@param[out] r_reindex_columns_first is mapping function of column layout of first operand
	 *		to (the merged) second operand column layout
	 *
	 *	@return Returns true on success, false on failure (the layouts couldn't be merged).
	 *
	 *	@note This function throws std::bad_alloc.
	 */
	static bool Build_AdditionLayouts(const std::vector<TRow> &r_row_list_first,
		const std::vector<TColumn> &r_column_list_first, std::vector<TRow> &r_row_list_second,
		std::vector<TColumn> &r_column_list_second, std::vector<size_t> &r_reindex_rows_first,
		std::vector<size_t> &r_reindex_columns_first); // throw(std::bad_alloc)

#ifdef __UBER_BLOCK_MATRIX_HAVE_CSPARSE
	/**
	 *	@brief fills one existing block matrix cloumn from sparse matrix
	 *
	 *	@tparam _TIntType is the integer type assumed to be used in the sparse matrix structure
	 *
	 *	@param[in] p_sparse is the sparse matrix to be filled from
	 *		(there must be enough space in this between the selected
	 *		row and column and the lower-right corner of the matrix
	 *		to fit the sparse matrix in)
	 *	@param[out] r_t_col is the column to be filled (new blocks will be allocated,
	 *		old blocks will be modified - but not pre-cleared!)
	 *	@param[in] n_base_row_base is (zero-based) index of the first
	 *		row where the sparse data should be placed (in elements)
	 *	@param[in] n_base_col_base is (zero-based) index of the
	 *		first column where the sparse data should be placed (in elements)
	 *	@param[in] r_workspace is workspace for fast row id lookup
	 *		(must be either empty, or contain results for (a lower part)
	 *		of the same block matrix as this)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This allocates new blocks where necessary, otherwise the existing blocks will be
	 *		updated with sparse data (modified - but not pre-cleared!).
	 *	@note This uses workspace to lookup row ids. The lookup is assumed never to fail
	 *		(the matrix structure must be pre-allocated).
	 */
	template <class _TIntType>
	inline void From_Sparse_InnerLoop(const cs *p_sparse, TColumn &r_t_col,
		const size_t n_base_row_base, const size_t n_base_col_base,
		const std::vector<size_t> &r_workspace); // throw(std::bad_alloc)

#ifdef _OPENMP
	/**
	 *	@brief fills one existing block matrix cloumn from sparse matrix, in parallel
	 *
	 *	@tparam _TIntType is the integer type assumed to be used in the sparse matrix structure
	 *
	 *	@param[in] p_sparse is the sparse matrix to be filled from
	 *		(there must be enough space in this between the selected
	 *		row and column and the lower-right corner of the matrix
	 *		to fit the sparse matrix in)
	 *	@param[out] r_t_col is the column to be filled (new blocks will be allocated,
	 *		old blocks will be modified - but not pre-cleared!)
	 *	@param[in] n_base_row_base is (zero-based) index of the first
	 *		row where the sparse data should be placed (in elements)
	 *	@param[in] n_base_col_base is (zero-based) index of the
	 *		first column where the sparse data should be placed (in elements)
	 *	@param[in] r_workspace is workspace for fast row id lookup
	 *		(must be either empty, or contain results for (a lower part)
	 *		of the same block matrix as this)
	 *
	 *	@note This function throws std::bad_alloc.
	 *	@note This allocates new blocks where necessary, otherwise the existing blocks will be
	 *		updated with sparse data (modified - but not pre-cleared!).
	 *	@note This uses workspace to lookup row ids. The lookup is assumed never to fail
	 *		(the matrix structure must be pre-allocated).
	 *	@note This version contains a critical section around block data allocation
	 *		(only the data pool, not the insertion into r_t_col.block_list) and therefore
	 *		can run in parallel.
	 */
	template <class _TIntType>
	inline void From_Sparse_ParallelInnerLoop(const cs *p_sparse,
		/*omp_lock_t &t_mutex,*/ TColumn &r_t_col, const size_t n_base_row_base,
		const size_t n_base_col_base, const std::vector<size_t> &r_workspace); // throw(std::bad_alloc)
#endif // _OPENMP
#endif // __UBER_BLOCK_MATRIX_HAVE_CSPARSE
};

#include "BlockMatrix.inl"

#ifdef __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

#include "slam/BlockMatrixFBSUtil.h"

#ifndef __UBER_BLOCK_MATRIX_SUPRESS_FBS
#include "slam/BlockMatrixFBS.h"
#endif // !__UBER_BLOCK_MATRIX_SUPRESS_FBS
// include implementations of the FBS classes; do not put them here ...

#include "slam/BlockMatrixFBS.inl"
// implementation of CUberBlockMatrix::*_FBS() functions

#endif // __UBER_BLOCK_MATRIX_FIXED_BLOCK_SIZE_OPS

/** @} */ // end of group

#endif // !__UBER_BLOCK_MATRIX_INCLUDED
